{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEXICON HDC METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Basic Packages\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import nltk\n",
    "import string\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from nltk import sent_tokenize,word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from tabulate import tabulate\n",
    "from os import listdir\n",
    "from pandas import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Insights and Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Stanford IMBD Movie Review Dataset...\n",
      "Total number of Samples In Dataset: 50000\n"
     ]
    }
   ],
   "source": [
    "### Function: Collects sample data from files in Stanford Dataset Subfolders\n",
    "    ## Inputs: folderpath: Path to Desired Folder\n",
    "    ##         sentiment:  Sentiment Value (0 or 1)\n",
    "    ## Output: df: Pandas Dataframe of all Sample Data found in desired folder\n",
    "def stanfordDatasetFolderDataLoader(folderpath, sentiment):\n",
    "    file_list=listdir(folderpath)\n",
    "\n",
    "    df = pd.DataFrame(columns = ['Review', 'Sentiment'])\n",
    "    for file in file_list:\n",
    "        filepath=folderpath + file\n",
    "        f = open(filepath,'r', encoding=\"utf-8\")\n",
    "        sample = f.read()\n",
    "        f.close()\n",
    "        df = df.append({'Review' : sample, 'Sentiment' : sentiment}, ignore_index = True)\n",
    "    return df\n",
    "\n",
    "# Choose Dataset (0: Sentiment140, 1: Stanford IMBD Dataset)\n",
    "dataset = 1\n",
    "\n",
    "# Load Dataset\n",
    "if (dataset==0):\n",
    "    print('Using Sentiment140 Twitter Dataset...')\n",
    "    # Read in Sentiment140 data from CSV\n",
    "    df = pd.read_csv('./Sentiment140_Tweets/data.csv')\n",
    "    df.columns =['Sentiment', 'IDs', 'Date', 'Flag', 'User', 'Tweet']\n",
    "else:\n",
    "    print('Using Stanford IMBD Movie Review Dataset...')\n",
    "    # Read in Training Stanford IMBD Movie Review data from subfolders\n",
    "    train_pos=stanfordDatasetFolderDataLoader('./StanfordMovie/train/pos/',1)\n",
    "    train_neg=stanfordDatasetFolderDataLoader('./StanfordMovie/train/neg/',0)\n",
    "    train_df=pd.concat([train_pos, train_neg], axis=0)\n",
    "    \n",
    "    # Read in Testing Stanford IMBD Movie Review data from subfolders\n",
    "    test_pos=stanfordDatasetFolderDataLoader('./StanfordMovie/test/pos/',1)\n",
    "    test_neg=stanfordDatasetFolderDataLoader('./StanfordMovie/test/neg/',0)\n",
    "    test_df=pd.concat([test_pos, test_neg], axis=0)\n",
    "\n",
    "    df=pd.concat([train_df, test_df], axis=0)\n",
    "print('Total number of Samples In Dataset:', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset in Use has No NULL values.\n",
      "Dataset Length after Cleanup: 50000\n"
     ]
    }
   ],
   "source": [
    "# Dataset Cleanup:\n",
    "\n",
    "# Sentiment140 Sentiment Clean Up\n",
    "if dataset==0:\n",
    "    # Replace Sentiment of 4 (Positive) with 1\n",
    "    df[\"Sentiment\"].replace({4: 1}, inplace=True)\n",
    "    # Eliminate Neutral Tweets, if any\n",
    "    df = df[df['Sentiment'] != 2]\n",
    "\n",
    "# Check for Null Values\n",
    "if ( not df.isnull().values.any() ):\n",
    "    print(\"Dataset in Use has No NULL values.\")\n",
    "else:\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "print(\"Dataset Length after Cleanup:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11841</th>\n",
       "      <td>Often tagged as a comedy, The Man In The White...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7102</th>\n",
       "      <td>After Chaplin made one of his best films: Doug...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8019</th>\n",
       "      <td>I think the movie was one sided I watched it r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>I have fond memories of watching this visually...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5142</th>\n",
       "      <td>This episode had potential. The basic premise ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9869</th>\n",
       "      <td>Reading the various external reviews of Roger ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4692</th>\n",
       "      <td>To soccer fans every where -- stay away from t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12143</th>\n",
       "      <td>This is such a great movie to watch with young...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9963</th>\n",
       "      <td>have just got back from seeing this brilliantl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10833</th>\n",
       "      <td>This film aka \"the four hundred blows\" is a mi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Review Sentiment\n",
       "11841  Often tagged as a comedy, The Man In The White...         1\n",
       "7102   After Chaplin made one of his best films: Doug...         0\n",
       "8019   I think the movie was one sided I watched it r...         0\n",
       "747    I have fond memories of watching this visually...         1\n",
       "5142   This episode had potential. The basic premise ...         0\n",
       "...                                                  ...       ...\n",
       "9869   Reading the various external reviews of Roger ...         1\n",
       "4692   To soccer fans every where -- stay away from t...         0\n",
       "12143  This is such a great movie to watch with young...         1\n",
       "9963   have just got back from seeing this brilliantl...         1\n",
       "10833  This film aka \"the four hundred blows\" is a mi...         0\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downsample the Dataset to 5,000 Total Samples\n",
    "if (dataset==0):\n",
    "    percentage = 0.003125\n",
    "else:\n",
    "    percentage = 0.1\n",
    "df_downsampled = df.sample(frac=percentage,random_state=0)\n",
    "df_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where Sentiment is Positive when Sentiment Value = 1 and Negative when Sentiment Value = 0\n"
     ]
    }
   ],
   "source": [
    "# Visualization of Dataset Sentiment Outcomes - Ensured Even Distribution of Outcomes\n",
    "sent_count = df_downsampled['Sentiment'].value_counts()\n",
    "plt.figure(figsize=(8, 5))\n",
    "w = 0.35  \n",
    "plt.bar(x=np.arange(len(sent_count)), height=sent_count, width = w)\n",
    "plt.xticks(np.arange(len(sent_count)), sent_count.index.tolist())\n",
    "\n",
    "if dataset==0:\n",
    "    plt.xlabel('Tweet Sentiment Value')\n",
    "    plt.ylabel('Tweet Sentiment Value Count')\n",
    "    plt.title('Sentiment140 Twitter Dataset Sentiment Value Frequency')\n",
    "else:\n",
    "    plt.xlabel('Movie Review Sentiment Value')\n",
    "    plt.ylabel('Movie Review Sentiment Value Count')\n",
    "    plt.title('Stanford IMBD Movie Review Dataset Sentiment Value Frequency')\n",
    "plt.show()\n",
    "print('Where Sentiment is Positive when Sentiment Value = 1 and Negative when Sentiment Value = 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like standard sly flick involve top action unbelievable stunt unbelievable is not intended be complimentary here retarded dialogue love steaming pile mountain goat dung i had high hope based trailer i thought stalone wa going be forced hasbeen day yield smarter people make action film place credible hero credible situation where story setting believable action prevail i crave action is least close enough reality imagine fear excitement come such event limited knowledge hypothermia effect rendered least scene laughably ridiculous judge dredd is only better know going theater are going see comic book made movie character setting everything else are comparison anything encounter cliffhanger other hand turn mountain climbing guide rambo say yo adrian',\n",
       " 'outing certainly doe nt live up predecessor doe have more share memorable moment personal favorite just laying waste city block videodisc cannon see close nimoy s face single tear shed left eye know point nimoy is more just killing machine viewer nt help be pulled emotional turmoil understand previously flat affect wa only facade absolute brilliance sex scene display nice balance carnal not pornographic afterwards i felt i had pretty good understanding how work magnavision videodisc player too bad have nt produced year',\n",
       " 'bad sequel s real when first movie wa very very bad have be fool make sequel worse actor worse scenario worse special effect worse movie is history bad history i give half laugh',\n",
       " 'most part lethally dull venture naach is dancerchoreographer antara mali regard kind auteur petty commercial compromise people including actor beau abhishek bachchan build career nice idea only turn ha most howlariously bad concept costume dance movement stuff rotten enough make even force ultracheap south indian potboiler squirm severe embarrassment br story follows yawninducing predictable pattern dancer beau meet have affair yes sex included beau get success spurns attempt help beau throw attitude walk other dancer meet most unlikely director want do want incongruous effort bag even more incongruous popularity beau s attempt reach are rebuffed climax where burst how nt live duh end br actually i did nt really expect film be much good so why did i watch i have antara mali fetish s actress imo sufficient talent reach grand height provided ha opportunity ala role film look far better paper actual execution ironic film s supposed be uncompromising character give credit put game effort shining well few scene actually ask actress i just wish movie had been more worthy abhishek bachchan also throw few good punch last scene salvaged somewhat simmering turn pleasantly reminds father br br only recommended sit humongous pile dogcrap return few grain good moment courtesy lead actor']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Function: Pre-Processes Samples\n",
    "    ## Inputs: samples:        Array of samples\n",
    "    ##         sample_results: Sentiment of Input Samples\n",
    "    ## Outputs: pre_procc_samps: Array of samples pre-processed\n",
    "    ##          pre_procc_res:   Array of results for pre-processed samples      \n",
    "def PreProcess(samples, sample_results):\n",
    "    pre_procc_samps = []\n",
    "    pre_procc_res=[]\n",
    "\n",
    "    # Storing all punctuations using RE library like !;,\"% etc\n",
    "    re_puncs = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # Lemmatizing object\n",
    "    lem = WordNetLemmatizer()\n",
    "\n",
    "    index=0\n",
    "    for sample in samples:\n",
    "        \n",
    "        # Replace Repeated Characters with 2 instance and get rid or URLs / Handles\n",
    "        sample=re.sub(r\"(\\w)\\1{2,}\", r\"\\1\\1\", str(sample))\n",
    "        sample = re.sub(r\"http\\S+\", \"\", str(sample))\n",
    "        sample = re.sub(r\"@\\S+\", \"\", str(sample))\n",
    "\n",
    "        # Get words in sample\n",
    "        words = word_tokenize(str(sample))\n",
    "\n",
    "        # Converting all characters to lower case\n",
    "        words_lower = [w.lower() for w in words]              \n",
    "\n",
    "        # Remove all punctuation\n",
    "        words_lower_no_punc = [re_puncs.sub('', w) for w in words_lower]\n",
    "\n",
    "        # Keep only alpha words\n",
    "        words_lower_alpha = [i for i in words_lower_no_punc if i.isalpha()]\n",
    "\n",
    "        # POS Tagging\n",
    "        pos_tagged_words = nltk.pos_tag(words_lower_alpha)\n",
    "        filtered_pos = [t[0] for t in pos_tagged_words if t[1] == \"NN\" or t[1] == \"NNS\" or t[1].startswith('J') or t[1].startswith('RB') or t[1].startswith('V') or t[1] == \"UH\" or t[1] == \"WRB\" or t[1] == \"POS\"]\n",
    "\n",
    "        # Doing Lemmatizing of words\n",
    "        words_lower_alpha_pos_lemma = [lem.lemmatize(w) for w in filtered_pos]\n",
    "\n",
    "        # Convert back to string and (possibly) one-hot encode tweet\n",
    "        pre_procc_str = ' '.join(words_lower_alpha_pos_lemma)\n",
    "        if (pre_procc_str != \"\"):\n",
    "            pre_procc_samps.append(pre_procc_str)\n",
    "            pre_procc_res.append(sample_results[index])\n",
    "        index=index+1\n",
    "        \n",
    "    return pre_procc_samps, pre_procc_res\n",
    "\n",
    "# Pre-Proccess the Dataset\n",
    "if dataset==0:\n",
    "    Xdf, Ydf = PreProcess(df_downsampled['Tweet'].to_numpy(), df_downsampled['Sentiment'].to_numpy())\n",
    "else:\n",
    "    Xdf, Ydf = PreProcess(df_downsampled['Review'].to_numpy(), df_downsampled['Sentiment'].to_numpy())\n",
    "\n",
    "# Get Final Train/Test Sets:\n",
    "TrainXdf,TestXdf, TrainYdf, TestYdf = train_test_split(Xdf, Ydf,test_size=.2, random_state=2)\n",
    "TrainYdf=np.array(TrainYdf)\n",
    "TestYdf=np.array(TestYdf)\n",
    "TrainXdf[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Training Initialization\n",
    "### Generate Lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function: Automatic Unigram and Bigram Lexicon Generation\n",
    "    ## Inputs: TrainXdf: Training Samples\n",
    "    ##         TrainYdf: Training Sample Results\n",
    "    ## Outputs: unigram_lexicon: Unigram Lexicon generated from Labelled Training Data\n",
    "    ##          bigram_lexicon:  Bigram Lexicon generated from Labelled Training Data\n",
    "def buildLexicons(TrainXdf,TrainYdf):\n",
    "    \n",
    "    # Initialization\n",
    "    unigram_pos_count_dict={}\n",
    "    unigram_neg_count_dict={}\n",
    "    unigram_lexicon={}\n",
    "\n",
    "    bigram_pos_count_dict={}\n",
    "    bigram_neg_count_dict={}\n",
    "    bigram_lexicon={}\n",
    "    \n",
    "    index=0\n",
    "\n",
    "    for tweet in TrainXdf:\n",
    "\n",
    "        # Get words in tweet\n",
    "        words = word_tokenize(str(tweet))\n",
    "\n",
    "        # Initialization\n",
    "        next_word_idx = 1\n",
    "        prev_unigrams=[]\n",
    "        prev_bigrams=[]\n",
    "\n",
    "        for word in words:\n",
    "    \n",
    "            # Unigram lexicon:\n",
    "            # Increment Positive Count\n",
    "            if TrainYdf[index]==1 and word not in prev_unigrams:\n",
    "                if word in unigram_pos_count_dict:\n",
    "                    unigram_pos_count_dict[word]=unigram_pos_count_dict[word]+1\n",
    "                else:\n",
    "                    unigram_pos_count_dict[word]=1\n",
    "                    unigram_neg_count_dict[word]=0\n",
    "                prev_unigrams.append(word)\n",
    "            # Increment Negative Count\n",
    "            elif TrainYdf[index]==0 and word not in prev_unigrams:\n",
    "                if word in unigram_neg_count_dict:\n",
    "                    unigram_neg_count_dict[word]=unigram_neg_count_dict[word]+1\n",
    "                else:\n",
    "                    unigram_neg_count_dict[word]=1\n",
    "                    unigram_pos_count_dict[word]=0\n",
    "                prev_unigrams.append(word)\n",
    "\n",
    "            # Bigram lexicon:\n",
    "            if (next_word_idx < len(words)):\n",
    "                bigram = word + \" \" + words[next_word_idx]\n",
    "                next_word_idx = next_word_idx + 1\n",
    "\n",
    "                # Increment Positive Count\n",
    "                if TrainYdf[index]==1 and bigram not in prev_bigrams:\n",
    "                    if bigram in bigram_pos_count_dict:\n",
    "                        bigram_pos_count_dict[bigram]=bigram_pos_count_dict[bigram]+1\n",
    "                    else:\n",
    "                        bigram_pos_count_dict[bigram]=1\n",
    "                        bigram_neg_count_dict[bigram]=0\n",
    "                    prev_bigrams.append(bigram)\n",
    "                # Increment Negative Count\n",
    "                elif TrainYdf[index]==0 and bigram not in prev_bigrams:\n",
    "                    if bigram in bigram_neg_count_dict:\n",
    "                        bigram_neg_count_dict[bigram]=bigram_neg_count_dict[bigram]+1\n",
    "                    else:\n",
    "                        bigram_neg_count_dict[bigram]=1\n",
    "                        bigram_pos_count_dict[bigram]=0\n",
    "                    prev_bigrams.append(bigram)\n",
    "\n",
    "        index=index+1\n",
    "    \n",
    "    # Calculate polarity score for each word and add pair to the unigram lexicon\n",
    "    for key in unigram_pos_count_dict.keys():\n",
    "        if ((unigram_pos_count_dict[key]+unigram_neg_count_dict[key]) >= .01*len(TrainYdf)):\n",
    "            pos_sent_score = unigram_pos_count_dict[key]/(unigram_pos_count_dict[key]+unigram_neg_count_dict[key])\n",
    "            if (pos_sent_score < 0.4 or pos_sent_score > 0.6):\n",
    "                polarity_score = 2*pos_sent_score-1\n",
    "                unigram_lexicon[key]=polarity_score\n",
    "\n",
    "    # Calculate polarity score for each word and add pair to the bigram lexicon\n",
    "    for key in bigram_pos_count_dict.keys():\n",
    "        if ((bigram_pos_count_dict[key]+bigram_neg_count_dict[key]) >= .01*len(TrainYdf)):\n",
    "            pos_sent_score = bigram_pos_count_dict[key]/(bigram_pos_count_dict[key]+bigram_neg_count_dict[key])\n",
    "            if (pos_sent_score < 0.4 or pos_sent_score > 0.6):\n",
    "                polarity_score = 2*pos_sent_score-1\n",
    "                bigram_lexicon[key]=polarity_score\n",
    "\n",
    "    return unigram_lexicon, bigram_lexicon\n",
    "\n",
    "# Automatic Unigram and Bigram Lexicon Generation\n",
    "auto_unigram_lexicon, auto_bigram_lexicon = buildLexicons(TrainXdf,TrainYdf)\n",
    "\n",
    "# Get WKWSCI Lexicon\n",
    "xls = ExcelFile('WKWSCISentimentLexicon_v1.1.xlsx')\n",
    "data = xls.parse(xls.sheet_names[3])\n",
    "WKWSCI_lexicon = data.set_index(\"term\")[\"sentiment\"].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexicon Value List Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function: Get list of possible lexicon values\n",
    "    ## Inputs: totalLexLevels: Total Number of Lexicon Score Values\n",
    "    ##         minLexVal:      Smallest Lexicon Score Value \n",
    "    ##         maxLexVal:      Largest Lexicon Score Value \n",
    "    ## Output: lexiconValueList: List of All possible Lexicon Values\n",
    "def getLexiconValueList(totalLexLevels, minLexVal, maxLexVal):\n",
    "    lexiconValueList = []\n",
    "    length = maxLexVal - minLexVal\n",
    "    gap = length / totalLexLevels\n",
    "\n",
    "    for val in range(totalLexLevels):\n",
    "        lexiconValueList.append(minLexVal + val*gap)\n",
    "\n",
    "    lexiconValueList.append(maxLexVal)\n",
    "    return lexiconValueList\n",
    "\n",
    "# Parameters:\n",
    "HV_dim = 10000\n",
    "minLexVal = -1\n",
    "maxLexVal = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item Memory Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function: Item Memory Generation\n",
    "    ## Inputs: totalLexLevels: Total Number of Lexicon Score Values\n",
    "    ##         minLexVal:      Smallest Lexicon Score Value \n",
    "    ##         HV_dim:         Dimension of HV\n",
    "    ## Output: itemMem: Item Memory containing HVs for each Possible Lexicon Score Value\n",
    "def itemMemGen(totalLexLevels, minLexVal, HV_dim):\n",
    "    itemMem = dict()\n",
    "    indexVector = range(HV_dim)\n",
    "    nextLevel = int((HV_dim/2/totalLexLevels))\n",
    "    change = int(HV_dim/2)\n",
    "    for level in range(totalLexLevels):\n",
    "        name = level\n",
    "        if(level == 0):\n",
    "            base = np.full(HV_dim, minLexVal)\n",
    "            toOne = np.random.permutation(indexVector)[:change]\n",
    "        else:\n",
    "            toOne = np.random.permutation(indexVector)[:nextLevel]\n",
    "        for index in toOne:\n",
    "            base[index] = base[index] * -1\n",
    "        itemMem[name] = copy.copy(base)\n",
    "    return itemMem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function: Binary search for Value in Lexicon Value List\n",
    "    ## Inputs: value:            Value Trying to Find in List\n",
    "    ##         lexiconValueList: List of Possible Lexicon Values\n",
    "    ## Output: ValueIndex: Index of Desired Value in Lexicon Value List\n",
    "def numToValueIndex(value, lexiconValueList):\n",
    "    if (value == lexiconValueList[-1]):\n",
    "        return len(lexiconValueList)-2\n",
    "    upperIndex = len(lexiconValueList) - 1\n",
    "    lowerIndex = 0\n",
    "    ValueIndex = 0\n",
    "    while (upperIndex > lowerIndex):\n",
    "        ValueIndex = int((upperIndex + lowerIndex)/2)\n",
    "        if (lexiconValueList[ValueIndex] <= value and lexiconValueList[ValueIndex+1] > value):\n",
    "            return ValueIndex\n",
    "        if (lexiconValueList[ValueIndex] > value):\n",
    "            upperIndex = ValueIndex\n",
    "            ValueIndex = int((upperIndex + lowerIndex)/2)\n",
    "        else:\n",
    "            lowerIndex = ValueIndex\n",
    "            ValueIndex = int((upperIndex + lowerIndex)/2)\n",
    "    return ValueIndex\n",
    "\n",
    "### Function: Normalizes WKWSCI Sentiment Score in the Range [-1, 1]\n",
    "    ## Input: value: WKWSCI value\n",
    "    ## Output: norm_val: Normalized WKWSCI value\n",
    "def normWKWSCI(value):\n",
    "    WKWSCI_max = 3\n",
    "    WKWSCI_min = -3\n",
    "    norm_val = 2 * ((value-WKWSCI_min)/(WKWSCI_max-WKWSCI_min))- 1\n",
    "    return norm_val\n",
    "\n",
    "### Function: Encodes a Sample into a HV using LEXICON HDC Approach\n",
    "    ## Inputs: sample:               Training Sample\n",
    "    ##         itemMem:              Generated Item Memory\n",
    "    ##         lexiconValueList:     List of Possible Lexicon Values \n",
    "    ##         encode_method:        Encode Method (0 - ADD, 1 - MULT)\n",
    "    ##         lex_type_flag:        Type of Lexicon Using (0 - UNIGRAM, 1 - BIGRAM)\n",
    "    ##         combo_flag:           Lexicon(s) Using (0 - WKWSCI, 1 - EITHER WKWSCI+AUTO or AUTO [DEPENDS ON LEX_TYPE] )\n",
    "    ##         WKWSCI_lexicon:       WKWSCI Lexicon\n",
    "    ##         auto_unigram_lexicon: Automatic Unigram Lexicon Generated\n",
    "    ##         auto_bigram_lexicon:  Automatic Bigram Lexicon Generated\n",
    "    ## Output: sample_HV: HV of inputted sample \n",
    "def encode(sample, itemMem, lexiconValueList, encode_method, lex_type_flag, combo_flag, WKWSCI_lexicon, auto_unigram_lexicon, auto_bigram_lexicon):\n",
    "    sample_HV = np.zeros(HV_dim, dtype='int32')\n",
    "    sample = word_tokenize(sample)\n",
    "    \n",
    "    next_word_idx=1\n",
    "    first=1\n",
    "\n",
    "    for word in sample:\n",
    "        if combo_flag==1:\n",
    "            if lex_type_flag==0:\n",
    "                if word in WKWSCI_lexicon:\n",
    "                    raw_lexicon_score = WKWSCI_lexicon[word]\n",
    "                    lexicon_score = normWKWSCI(raw_lexicon_score)\n",
    "                    if word in auto_unigram_lexicon:\n",
    "                        other_score = auto_unigram_lexicon[word]\n",
    "                        lexicon_score=0.5*(lexicon_score + other_score)\n",
    "                elif word in auto_unigram_lexicon:\n",
    "                    lexicon_score = auto_unigram_lexicon[word]\n",
    "                else:\n",
    "                    continue\n",
    "            elif next_word_idx < len(sample):\n",
    "                bigram = word + \" \" + sample[next_word_idx]\n",
    "                next_word_idx=next_word_idx+1\n",
    "                if bigram in auto_bigram_lexicon:\n",
    "                    lexicon_score = auto_bigram_lexicon[bigram]\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "        elif combo_flag==0 and lex_type_flag==0:\n",
    "            if word in WKWSCI_lexicon:\n",
    "                raw_lexicon_score = WKWSCI_lexicon[word]\n",
    "                lexicon_score = normWKWSCI(raw_lexicon_score)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        HV_score_index = numToValueIndex(lexicon_score, lexiconValueList)\n",
    "\n",
    "        if encode_method==0:   \n",
    "            if first==1:\n",
    "                sample_HV = itemMem[HV_score_index]\n",
    "                first=0\n",
    "            else:\n",
    "                sample_HV=sample_HV*itemMem[HV_score_index]\n",
    "        else:\n",
    "            sample_HV=sample_HV+itemMem[HV_score_index]\n",
    "    \n",
    "    sample_HV=sample_HV.flatten()\n",
    "    if encode_method==1:\n",
    "        HV_avg = np.average(sample_HV)\n",
    "        sample_HV[sample_HV > HV_avg] = 1\n",
    "        sample_HV[sample_HV < HV_avg] = -1\n",
    "        sample_HV[sample_HV == HV_avg] = 0\n",
    "\n",
    "    return sample_HV\n",
    "\n",
    "\n",
    "### Function: LEXICON HDC Training Function that creates an Associative Memory for the Model\n",
    "    ## Inputs: X:                    Training Samples\n",
    "    ##         Y:                    Outputs of Training Samples\n",
    "    ##         itemMem:              Generated Item Memory\n",
    "    ##         HV_dim:               Dimension of HV\n",
    "    ##         sent_count:           Number of Possible Sentiment Values\n",
    "    ##         lexiconValueList:     List of Possible Lexicon Values \n",
    "    ##         encode_method:        Encode Method (0 - ADD, 1 - MULT)\n",
    "    ##         lex_type_flag:        Type of Lexicon Using (0 - UNIGRAM, 1 - BIGRAM)\n",
    "    ##         combo_flag:           Lexicon(s) Using (0 - WKWSCI, 1 - EITHER WKWSCI+AUTO or AUTO [DEPENDS ON LEX_TYPE] )\n",
    "    ##         WKWSCI_lexicon:       WKWSCI Lexicon\n",
    "    ##         auto_unigram_lexicon: Automatic Unigram Lexicon Generated\n",
    "    ##         auto_bigram_lexicon:  Automatic Bigram Lexicon Generated\n",
    "    ## Output: assocMem: Associative Memory \n",
    "def train(X, Y, itemMem, HV_dim, sent_count, lexiconValueList, encode_method, lex_type_flag, combo_flag, WKWSCI_lexicon, auto_unigram_lexicon, auto_bigram_lexicon):\n",
    "    assocMem = np.zeros((sent_count, HV_dim), dtype='int32')\n",
    "    sample_idx = 0\n",
    "    \n",
    "    for sample in X:\n",
    "        sample_HV = encode(sample, itemMem, lexiconValueList, encode_method, lex_type_flag, combo_flag, WKWSCI_lexicon, auto_unigram_lexicon, auto_bigram_lexicon)\n",
    "        assocMem[Y[sample_idx]] = np.add(assocMem[Y[sample_idx]], sample_HV)\n",
    "        sample_idx += 1\n",
    "    \n",
    "    return assocMem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function: Compares Input HV to Class HVs and Returns the Predicted Class\n",
    "    ## Inputs: assocMem: Model's Associative Memory\n",
    "    ##         inputHV:  Encoded HV of a sample\n",
    "    ## Output: pred: the predicted class\n",
    "def get_prediction(assocMem, inputHV):\n",
    "    pred = assocMem[0]\n",
    "    maximum = np.NINF\n",
    "\n",
    "    for index in range(len(assocMem)):\n",
    "        similarity = cosine_similarity([inputHV, assocMem[index]])[0][1]  \n",
    "        if (similarity > maximum):\n",
    "            pred = index\n",
    "            maximum = similarity\n",
    "\n",
    "    return pred\n",
    "\n",
    "### Function: Tests the LEXICON HDC Model and Returns Accuracy of Model\n",
    "    ## Inputs: itemMem:          Generated Item Memory\n",
    "    ##         assocMem:         Model's Associative Memory\n",
    "    ##         TestXdf:          Test Samples\n",
    "    ##         TextYdf:          Sentiment of Test Samples\n",
    "    ##         lexiconValueList: List of Possible Lexicon Values \n",
    "    ##         encode_method:    Encode Method (0 - ADD, 1 - MULT)\n",
    "    ##         lex_type_flag:        Type of Lexicon Using (0 - UNIGRAM, 1 - BIGRAM)\n",
    "    ##         combo_flag:           Lexicon(s) Using (0 - WKWSCI, 1 - EITHER WKWSCI+AUTO or AUTO [DEPENDS ON LEX_TYPE] )\n",
    "    ##         WKWSCI_lexicon:   WKWSCI Lexicon\n",
    "    ##         auto_unigram_lexicon: Automatic Unigram Lexicon Generated\n",
    "    ##         auto_bigram_lexicon:  Automatic Bigram Lexicon Generated\n",
    "    ## Output: accuracy: Accuracy of the Model\n",
    "def test(itemMem, assocMem, TestXdf, TestYdf, lexiconValueList, encode_method, lex_type_flag, combo_flag, WKWSCI_lexicon, auto_unigram_lexicon, auto_bigram_lexicon):\n",
    "    true_pos_count=0\n",
    "    false_pos_count=0\n",
    "    correct_count = 0\n",
    "\n",
    "    for index in range(len(TestXdf)):\n",
    "        prediction = get_prediction(assocMem, encode(TestXdf[index], itemMem, lexiconValueList, encode_method, lex_type_flag, combo_flag, WKWSCI_lexicon, auto_unigram_lexicon, auto_bigram_lexicon))\n",
    "        if (TestYdf[index] == prediction):\n",
    "            correct_count += 1\n",
    "            if prediction==1:\n",
    "                true_pos_count = true_pos_count + 1\n",
    "        elif prediction==1:\n",
    "            false_pos_count = false_pos_count + 1\n",
    "    \n",
    "    accuracy = (correct_count / len(TestYdf)) * 100\n",
    "    if (true_pos_count+false_pos_count) != 0:\n",
    "        precision = (true_pos_count/ (true_pos_count+false_pos_count)) * 100\n",
    "    else:\n",
    "        precision=0\n",
    "    return accuracy, precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Hyperparameter Search\n",
    "### One-Shot Training/Accuracy of Various Sets of Hyperparameter Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Various Hyperparameter Sets:\n",
      "LEXICON(S) USED     TYPE OF LEXICON(S)      NUMBER OF LEXICON VALUES  ENCODE METHOD      ONE-SHOT ACCURACY    ONE-SHOT-PRECISION    TRAINING TIME (s)    NUMBER OF TRAINING SAMPLES    TESTING TIME (s)    NUMBER OF TESTING SAMPLES\n",
      "------------------  --------------------  --------------------------  ---------------  -------------------  --------------------  -------------------  ----------------------------  ------------------  ---------------------------\n",
      "WKWSCI              UNIGRAM                                       75  [0, 1]                          57.7               55.8767              9.90713                          4000             3.56045                         1000\n",
      "WKWSCI              UNIGRAM                                       75  [0, 1]                          51.5                0                  10.1157                           4000             3.65425                         1000\n",
      "WKWSCI              UNIGRAM                                      100  [0, 1]                          57.3               55.5773              9.92442                          4000             3.61539                         1000\n",
      "WKWSCI              UNIGRAM                                      100  [0, 1]                          51.6                0                  10.2236                           4000             3.95943                         1000\n",
      "WKWSCI              UNIGRAM                                      150  [0, 1]                          57.7               55.8767             10.1039                           4000             3.65625                         1000\n",
      "WKWSCI              UNIGRAM                                      150  [0, 1]                          51.6                0                  10.2464                           4000             3.77497                         1000\n",
      "WKWSCI + AUTOMATIC  UNIGRAM                                       75  [0, 1]                          64.1               61.9962             10.4512                           4000             3.75499                         1000\n",
      "WKWSCI + AUTOMATIC  UNIGRAM                                       75  [0, 1]                          51.6               50                  10.6964                           4000             3.92599                         1000\n",
      "WKWSCI + AUTOMATIC  UNIGRAM                                      100  [0, 1]                          63.9               61.6698             10.4345                           4000             3.88415                         1000\n",
      "WKWSCI + AUTOMATIC  UNIGRAM                                      100  [0, 1]                          48.7               48.5456             10.8381                           4000             3.87008                         1000\n",
      "WKWSCI + AUTOMATIC  UNIGRAM                                      150  [0, 1]                          61.7               59.8826             11.4556                           4000             4.17785                         1000\n",
      "WKWSCI + AUTOMATIC  UNIGRAM                                      150  [0, 1]                          48.7               48.5456             10.8258                           4000             3.86167                         1000\n",
      "AUTOMATIC           BIGRAM                                        75  [0, 1]                          62.6               64.9457              3.84488                          4000             1.87499                         1000\n",
      "AUTOMATIC           BIGRAM                                        75  [0, 1]                          71.6               80.8642              5.06196                          4000             2.25598                         1000\n",
      "AUTOMATIC           BIGRAM                                       100  [0, 1]                          62                 64.2077              4.02131                          4000             1.90892                         1000\n",
      "AUTOMATIC           BIGRAM                                       100  [0, 1]                          71.6               80.8642              5.19316                          4000             2.21209                         1000\n",
      "AUTOMATIC           BIGRAM                                       150  [0, 1]                          62.6               64.9457              3.89661                          4000             1.91013                         1000\n",
      "AUTOMATIC           BIGRAM                                       150  [0, 1]                          71.6               80.8642              5.22691                          4000             2.26196                         1000\n",
      "\n",
      "Encode Method Key: 0 for ADD/AVG, 1 for MULT\n",
      "\n",
      "Best Hyperparameters: LEXICON(S) USED:  AUTOMATIC , TYPE OF LEXICON(S):  BIGRAM , NUMBER OF LEXICON VALUES:  75 , ENCODE METHOD:  1\n",
      "Best One-Shot Accuracy:  71.6\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "lex_combos = [0, 1] # 0 for WKWSCI, 1 for WKWSCI+AUTO\n",
    "auto_lexicon_type = [0, 1] # 0 for UNIGRAM, 1 for BIGRAM\n",
    "num_lex_values = [75, 100, 150]\n",
    "encode_method = [0, 1] # 0 for ADD, 1 for MULT \n",
    "\n",
    "# Optimal Result Initialization\n",
    "best_acc=0\n",
    "lex_combos_best=0\n",
    "auto_lexicon_type_best=0\n",
    "num_lex_values_best=0\n",
    "encode_method_best=0\n",
    "output_best_lex_used=\"\"\n",
    "output_best_lex_type=\"\"\n",
    "lexiconValueList_best=[]\n",
    "best_assocMem=[]\n",
    "itemMem_best=[]\n",
    "\n",
    "# Generate Item Memories and Lexicon Value Lists Once for Each Option of Number of Lexicon Values\n",
    "ItemMem_75, ItemMem_100, ItemMem_150=[], [], []\n",
    "LexValueList_75, LexValueList_100, LexValueList_150=[], [], []\n",
    "for num_lex in num_lex_values:\n",
    "    if num_lex==75:\n",
    "        ItemMem_75 = itemMemGen(num_lex, minLexVal, HV_dim)\n",
    "        LexValueList_75 = getLexiconValueList(num_lex, minLexVal, maxLexVal)\n",
    "    elif num_lex==100:\n",
    "        ItemMem_100 = itemMemGen(num_lex, minLexVal, HV_dim)\n",
    "        LexValueList_100 = getLexiconValueList(num_lex, minLexVal, maxLexVal)\n",
    "    elif num_lex==150:\n",
    "        ItemMem_150 = itemMemGen(num_lex, minLexVal, HV_dim)\n",
    "        LexValueList_150 = getLexiconValueList(num_lex, minLexVal, maxLexVal)\n",
    "\n",
    "# Generate Table Initialization\n",
    "table_data=[]\n",
    "col_names = [\"LEXICON(S) USED\", \"TYPE OF LEXICON(S)\", \"NUMBER OF LEXICON VALUES\", \"ENCODE METHOD\", \"ONE-SHOT ACCURACY\", \"ONE-SHOT-PRECISION\", \"TRAINING TIME (s)\", \"NUMBER OF TRAINING SAMPLES\", \"TESTING TIME (s)\", \"NUMBER OF TESTING SAMPLES\"]\n",
    "\n",
    "for auto_lex_type in auto_lexicon_type:\n",
    "    for combo in lex_combos:            \n",
    "        if auto_lex_type==1 and combo==0:\n",
    "            continue\n",
    "\n",
    "        for num_lex in num_lex_values:\n",
    "            # Set ItemMem and Lexicon Value List\n",
    "            if num_lex==75:\n",
    "                itemMem = copy.copy(ItemMem_75)\n",
    "                lexiconValueList = copy.copy(LexValueList_75)\n",
    "            elif num_lex==100:\n",
    "                itemMem = copy.copy(ItemMem_100)\n",
    "                lexiconValueList = copy.copy(LexValueList_100)\n",
    "            elif num_lex==150:\n",
    "                itemMem = copy.copy(ItemMem_150)\n",
    "                lexiconValueList = copy.copy(LexValueList_150)\n",
    "\n",
    "            for method in encode_method:\n",
    "\n",
    "                # Train Model (i.e. Generate Model's Associative Memory)\n",
    "                t0=time.time()\n",
    "                assocMem = train(TrainXdf, TrainYdf, itemMem, HV_dim, len(sent_count), lexiconValueList, method, auto_lex_type, combo, WKWSCI_lexicon, auto_unigram_lexicon, auto_bigram_lexicon)\n",
    "                t1=time.time()\n",
    "                train_time = t1-t0\n",
    "\n",
    "                # One-Shot Training Results\n",
    "                t0=time.time()\n",
    "                one_shot_accuracy, one_shot_precision =test(itemMem, assocMem, TestXdf, TestYdf, lexiconValueList, method, auto_lex_type, combo, WKWSCI_lexicon, auto_unigram_lexicon, auto_bigram_lexicon)\n",
    "                t1=time.time()\n",
    "                test_time = t1-t0\n",
    "\n",
    "                if combo==1:\n",
    "                    if auto_lex_type==0:\n",
    "                        # Collect Data for Table\n",
    "                        data = [\"WKWSCI + AUTOMATIC\", \"UNIGRAM\", num_lex, encode_method, one_shot_accuracy, one_shot_precision, train_time, len(TrainYdf), test_time, len(TestYdf)]\n",
    "                        lex_used=\"WKWSCI + AUTOMATIC\"\n",
    "                        lex_type=\"UNIGRAM\"\n",
    "                    else:\n",
    "                        # Collect Data for Table\n",
    "                        data = [\"AUTOMATIC\", \"BIGRAM\", num_lex, encode_method, one_shot_accuracy, one_shot_precision, train_time, len(TrainYdf), test_time, len(TestYdf)]\n",
    "                        lex_used=\"AUTOMATIC\"\n",
    "                        lex_type=\"BIGRAM\"\n",
    "                elif combo==0:\n",
    "                    # Collect Data for Table\n",
    "                    data = [\"WKWSCI\", \"UNIGRAM\", num_lex, encode_method, one_shot_accuracy, one_shot_precision, train_time, len(TrainYdf), test_time, len(TestYdf)]\n",
    "                    lex_used=\"WKWSCI\"\n",
    "                    lex_type=\"UNIGRAM\"\n",
    "\n",
    "                # Add Data to Table\n",
    "                table_data.append(data)\n",
    "\n",
    "                if one_shot_accuracy>best_acc:\n",
    "                    best_acc=one_shot_accuracy\n",
    "                    best_assocMem=copy.copy(assocMem)\n",
    "                    itemMem_best=copy.copy(itemMem)\n",
    "                    lexiconValueList_best=copy.copy(lexiconValueList)\n",
    "                    \n",
    "                    lex_combos_best=combo\n",
    "                    auto_lexicon_type_best=auto_lex_type\n",
    "                    num_lex_values_best=num_lex\n",
    "                    encode_method_best=method\n",
    "\n",
    "                    output_best_lex_used=lex_used\n",
    "                    output_best_lex_type=lex_type\n",
    "\n",
    "# Get Necessary Components for Best Model\n",
    "assocMem=copy.copy(best_assocMem)\n",
    "itemMem=copy.copy(itemMem_best)\n",
    "lexiconValueList=copy.copy(lexiconValueList_best)\n",
    "encode_method=encode_method_best\n",
    "lexicon_type=auto_lexicon_type_best\n",
    "combo=lex_combos_best\n",
    "\n",
    "# Save Results to File\n",
    "df=pd.DataFrame(table_data, columns=col_names)\n",
    "filepath=\"./Results/HyperparameterResults/LEXICON_\" + str(dataset) +\".csv\"\n",
    "df.to_csv(filepath)\n",
    "            \n",
    "print(\"Results of Various Hyperparameter Sets:\")\n",
    "print(tabulate(table_data, headers=col_names, tablefmt=\"simple\"))\n",
    "print(\"\\nEncode Method Key: 0 for ADD/AVG, 1 for MULT\\n\")\n",
    "print(\"Best Hyperparameters: LEXICON(S) USED: \", output_best_lex_used, \", TYPE OF LEXICON(S): \", output_best_lex_type, \", NUMBER OF LEXICON VALUES: \", num_lex_values_best, \", ENCODE METHOD: \", encode_method_best)\n",
    "print(\"Best One-Shot Accuracy: \", best_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retraining Optimal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Retraining Model w Learning Parameter:  20  Epochs --------\n",
      "Epoch  1 :  71.7\n",
      "Epoch  2 :  70.5\n",
      "Epoch  3 :  69.8\n",
      "Epoch  4 :  68.4\n",
      "Epoch  5 :  72.0\n",
      "Epoch  6 :  71.7\n",
      "Epoch  7 :  67.60000000000001\n",
      "Epoch  8 :  67.10000000000001\n",
      "Epoch  9 :  65.9\n",
      "Epoch  10 :  67.5\n",
      "Epoch  11 :  72.0\n",
      "Epoch  12 :  67.10000000000001\n",
      "Epoch  13 :  68.2\n",
      "Epoch  14 :  70.19999999999999\n",
      "Epoch  15 :  71.5\n",
      "Epoch  16 :  71.0\n",
      "Epoch  17 :  71.2\n",
      "Epoch  18 :  71.3\n",
      "Epoch  19 :  71.1\n",
      "Epoch  20 :  72.1\n",
      "-------- Retraining Model without Learning Parameter:  20  Epochs --------\n",
      "Epoch  1 :  71.8\n",
      "Epoch  2 :  68.60000000000001\n",
      "Epoch  3 :  73.0\n",
      "Epoch  4 :  71.2\n",
      "Epoch  5 :  71.1\n",
      "Epoch  6 :  69.3\n",
      "Epoch  7 :  68.89999999999999\n",
      "Epoch  8 :  67.5\n",
      "Epoch  9 :  70.5\n",
      "Epoch  10 :  70.0\n",
      "Epoch  11 :  68.10000000000001\n",
      "Epoch  12 :  69.89999999999999\n",
      "Epoch  13 :  67.10000000000001\n",
      "Epoch  14 :  69.89999999999999\n",
      "Epoch  15 :  63.2\n",
      "Epoch  16 :  70.39999999999999\n",
      "Epoch  17 :  66.60000000000001\n",
      "Epoch  18 :  69.1\n",
      "Epoch  19 :  69.19999999999999\n",
      "Epoch  20 :  67.4\n"
     ]
    }
   ],
   "source": [
    "### Function: VECTORIZED HDC Re-Training Function that creates a New Associative Memory for the Model\n",
    "    ## Inputs: X:                    Training Samples\n",
    "    ##         Y:                    Outputs of Training Samples\n",
    "    ##         itemMem:              Generated Item Memory\n",
    "    ##         assocMem:             Associative Memory of Current Model\n",
    "    ##         lexiconValueList:     List of Possible Lexicon Values \n",
    "    ##         encode_method:        Encode Method (0 - ADD, 1 - MULT)\n",
    "    ##         lex_type_flag:        Type of Lexicon Using (0 - UNIGRAM, 1 - BIGRAM)\n",
    "    ##         combo_flag:           Lexicon(s) Using (0 - WKWSCI, 1 - EITHER WKWSCI+AUTO or AUTO [DEPENDS ON LEX_TYPE] )\n",
    "    ##         WKWSCI_lexicon:       WKWSCI Lexicon\n",
    "    ##         auto_unigram_lexicon: Automatic Unigram Lexicon Generated\n",
    "    ##         auto_bigram_lexicon:  Automatic Bigram Lexicon Generated\n",
    "    ##         alpha:                Learning Rate Parameter\n",
    "    ## Output: assocMem: New Associative Memory\n",
    "def retrain(X, Y, itemMem, assocMem, lexiconValueList, encode_method, lex_type_flag, combo_flag, WKWSCI_lexicon, auto_unigram_lexicon, auto_bigram_lexicon, alpha):\n",
    "    sample_index = 0\n",
    "    for sample in X:\n",
    "        sample_HV = encode(sample, itemMem, lexiconValueList, encode_method, lex_type_flag, combo_flag, WKWSCI_lexicon, auto_unigram_lexicon, auto_bigram_lexicon)\n",
    "        prediction = get_prediction(assocMem, sample_HV)\n",
    "        if prediction != Y[sample_index]:\n",
    "            assocMem[Y[sample_index]] = np.add(assocMem[Y[sample_index]], alpha * sample_HV)\n",
    "            assocMem[prediction] = np.subtract(assocMem[prediction], alpha * sample_HV)\n",
    "        sample_index += 1\n",
    "    return assocMem\n",
    "\n",
    "# Re-Train Optimal Model with Learning Parameter\n",
    "learningparam_results=[]\n",
    "num_epochs = 20\n",
    "print('-------- Retraining Model w Learning Parameter: ', num_epochs, ' Epochs --------')\n",
    "for epoch in range(num_epochs):\n",
    "    assocMem = retrain(TrainXdf, TrainYdf, itemMem, assocMem, lexiconValueList, encode_method, lexicon_type, combo, WKWSCI_lexicon, auto_unigram_lexicon, auto_bigram_lexicon, alpha = num_epochs - epoch)\n",
    "    acc, prec = test(itemMem, assocMem, TestXdf, TestYdf, lexiconValueList, encode_method, lexicon_type, combo, WKWSCI_lexicon, auto_unigram_lexicon, auto_bigram_lexicon)\n",
    "    print('Epoch ', (epoch+1), ': ', acc)\n",
    "    learningparam_results.append([acc,prec])\n",
    "\n",
    "# Re-Train Optimal Model without Learning Parameter\n",
    "assocMem=copy.copy(best_assocMem)\n",
    "no_learningparam_results=[]\n",
    "print('-------- Retraining Model without Learning Parameter: ', num_epochs, ' Epochs --------')\n",
    "for epoch in range(num_epochs):\n",
    "    assocMem = retrain(TrainXdf, TrainYdf, itemMem, assocMem, lexiconValueList, encode_method, lexicon_type, combo, WKWSCI_lexicon, auto_unigram_lexicon, auto_bigram_lexicon, alpha = 1)\n",
    "    acc, prec = test(itemMem, assocMem, TestXdf, TestYdf, lexiconValueList, encode_method, lexicon_type, combo, WKWSCI_lexicon, auto_unigram_lexicon, auto_bigram_lexicon)\n",
    "    print('Epoch ', (epoch+1), ': ', acc)\n",
    "    no_learningparam_results.append([acc,prec])\n",
    "\n",
    "# Save All Results to Files\n",
    "col_name=[\"Accuracy\", \"Precision\"]\n",
    "\n",
    "df_lp=pd.DataFrame(learningparam_results, columns=col_name)\n",
    "filepath=\"./Results/EpochResults_LearningParam/LEXICON_\" + str(dataset) + \".csv\"\n",
    "df_lp.to_csv(filepath)\n",
    "\n",
    "df_nlp=pd.DataFrame(no_learningparam_results, columns=col_name)\n",
    "filepath=\"./Results/EpochResults_NoLearningParam/LEXICON_\" + str(dataset) + \".csv\"\n",
    "df_nlp.to_csv(filepath)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "071eaeccc96c6410cecdb330bf8e8ae0267d24b86e05481c728d399cbe7cbc33"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('aml': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
