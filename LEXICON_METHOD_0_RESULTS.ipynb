{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEXICON HDC METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Basic Packages\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import nltk\n",
    "import string\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from nltk import sent_tokenize,word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from tabulate import tabulate\n",
    "from os import listdir\n",
    "from pandas import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Insights and Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Sentiment140 Twitter Dataset...\n",
      "Total number of Samples In Dataset: 1599999\n"
     ]
    }
   ],
   "source": [
    "### Function: Collects sample data from files in Stanford Dataset Subfolders\n",
    "    ## Inputs: folderpath: Path to Desired Folder\n",
    "    ##         sentiment:  Sentiment Value (0 or 1)\n",
    "    ## Output: df: Pandas Dataframe of all Sample Data found in desired folder\n",
    "def stanfordDatasetFolderDataLoader(folderpath, sentiment):\n",
    "    file_list=listdir(folderpath)\n",
    "\n",
    "    df = pd.DataFrame(columns = ['Review', 'Sentiment'])\n",
    "    for file in file_list:\n",
    "        filepath=folderpath + file\n",
    "        f = open(filepath,'r', encoding=\"utf-8\")\n",
    "        sample = f.read()\n",
    "        f.close()\n",
    "        df = df.append({'Review' : sample, 'Sentiment' : sentiment}, ignore_index = True)\n",
    "    return df\n",
    "\n",
    "# Choose Dataset (0: Sentiment140, 1: Stanford IMBD Dataset)\n",
    "dataset = 0\n",
    "\n",
    "# Load Dataset\n",
    "if (dataset==0):\n",
    "    print('Using Sentiment140 Twitter Dataset...')\n",
    "    # Read in Sentiment140 data from CSV\n",
    "    df = pd.read_csv('./Sentiment140_Tweets/data.csv')\n",
    "    df.columns =['Sentiment', 'IDs', 'Date', 'Flag', 'User', 'Tweet']\n",
    "else:\n",
    "    print('Using Stanford IMBD Movie Review Dataset...')\n",
    "    # Read in Training Stanford IMBD Movie Review data from subfolders\n",
    "    train_pos=stanfordDatasetFolderDataLoader('./StanfordMovie/train/pos/',1)\n",
    "    train_neg=stanfordDatasetFolderDataLoader('./StanfordMovie/train/neg/',0)\n",
    "    train_df=pd.concat([train_pos, train_neg], axis=0)\n",
    "    \n",
    "    # Read in Testing Stanford IMBD Movie Review data from subfolders\n",
    "    test_pos=stanfordDatasetFolderDataLoader('./StanfordMovie/test/pos/',1)\n",
    "    test_neg=stanfordDatasetFolderDataLoader('./StanfordMovie/test/neg/',0)\n",
    "    test_df=pd.concat([test_pos, test_neg], axis=0)\n",
    "\n",
    "    df=pd.concat([train_df, test_df], axis=0)\n",
    "print('Total number of Samples In Dataset:', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset in Use has No NULL values.\n",
      "Dataset Length after Cleanup: 1599999\n"
     ]
    }
   ],
   "source": [
    "# Dataset Cleanup:\n",
    "\n",
    "# Sentiment140 Sentiment Clean Up\n",
    "if dataset==0:\n",
    "    # Replace Sentiment of 4 (Positive) with 1\n",
    "    df[\"Sentiment\"].replace({4: 1}, inplace=True)\n",
    "    # Eliminate Neutral Tweets, if any\n",
    "    df = df[df['Sentiment'] != 2]\n",
    "\n",
    "# Check for Null Values\n",
    "if ( not df.isnull().values.any() ):\n",
    "    print(\"Dataset in Use has No NULL values.\")\n",
    "else:\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "print(\"Dataset Length after Cleanup:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>IDs</th>\n",
       "      <th>Date</th>\n",
       "      <th>Flag</th>\n",
       "      <th>User</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1016244</th>\n",
       "      <td>1</td>\n",
       "      <td>1881672289</td>\n",
       "      <td>Fri May 22 05:16:44 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>viry_trivium</td>\n",
       "      <td>Happy birthday, sister!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1303317</th>\n",
       "      <td>1</td>\n",
       "      <td>2009051656</td>\n",
       "      <td>Tue Jun 02 15:04:22 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Earlthedog</td>\n",
       "      <td>Just finished eating supper and now I am attac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576684</th>\n",
       "      <td>0</td>\n",
       "      <td>2211886069</td>\n",
       "      <td>Wed Jun 17 13:24:27 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>StefyyMarie</td>\n",
       "      <td>i hate love right now.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837327</th>\n",
       "      <td>1</td>\n",
       "      <td>1558734942</td>\n",
       "      <td>Sun Apr 19 09:15:07 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tezzer57</td>\n",
       "      <td>Photo fest in LDN, Tudor feast last night, don...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985344</th>\n",
       "      <td>1</td>\n",
       "      <td>1834470136</td>\n",
       "      <td>Mon May 18 03:03:30 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>dave_sherratt</td>\n",
       "      <td>@piercedbrat happy bday for tomoz, all the bes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1369789</th>\n",
       "      <td>1</td>\n",
       "      <td>2050886442</td>\n",
       "      <td>Fri Jun 05 19:28:18 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>thaisprudencio</td>\n",
       "      <td>today was awesome!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587089</th>\n",
       "      <td>0</td>\n",
       "      <td>2216194514</td>\n",
       "      <td>Wed Jun 17 19:09:39 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>alwyshoutashley</td>\n",
       "      <td>I wish it would stop raining. I'm ready for th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46597</th>\n",
       "      <td>0</td>\n",
       "      <td>1677444411</td>\n",
       "      <td>Sat May 02 02:06:29 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>kasey79</td>\n",
       "      <td>@DannyGirlAlways Ok I still feel kind of bad t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1409132</th>\n",
       "      <td>1</td>\n",
       "      <td>2055829198</td>\n",
       "      <td>Sat Jun 06 10:01:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Bockman13</td>\n",
       "      <td>Hanging with Anna and Fernando!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984326</th>\n",
       "      <td>1</td>\n",
       "      <td>1834375043</td>\n",
       "      <td>Mon May 18 02:41:35 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AberdeenUK</td>\n",
       "      <td>Good morning everyone. FYI bought him Rock Ban...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sentiment         IDs                          Date      Flag  \\\n",
       "1016244          1  1881672289  Fri May 22 05:16:44 PDT 2009  NO_QUERY   \n",
       "1303317          1  2009051656  Tue Jun 02 15:04:22 PDT 2009  NO_QUERY   \n",
       "576684           0  2211886069  Wed Jun 17 13:24:27 PDT 2009  NO_QUERY   \n",
       "837327           1  1558734942  Sun Apr 19 09:15:07 PDT 2009  NO_QUERY   \n",
       "985344           1  1834470136  Mon May 18 03:03:30 PDT 2009  NO_QUERY   \n",
       "...            ...         ...                           ...       ...   \n",
       "1369789          1  2050886442  Fri Jun 05 19:28:18 PDT 2009  NO_QUERY   \n",
       "587089           0  2216194514  Wed Jun 17 19:09:39 PDT 2009  NO_QUERY   \n",
       "46597            0  1677444411  Sat May 02 02:06:29 PDT 2009  NO_QUERY   \n",
       "1409132          1  2055829198  Sat Jun 06 10:01:49 PDT 2009  NO_QUERY   \n",
       "984326           1  1834375043  Mon May 18 02:41:35 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    User                                              Tweet  \n",
       "1016244     viry_trivium                           Happy birthday, sister!   \n",
       "1303317       Earlthedog  Just finished eating supper and now I am attac...  \n",
       "576684       StefyyMarie                            i hate love right now.   \n",
       "837327          tezzer57  Photo fest in LDN, Tudor feast last night, don...  \n",
       "985344     dave_sherratt  @piercedbrat happy bday for tomoz, all the bes...  \n",
       "...                  ...                                                ...  \n",
       "1369789   thaisprudencio                                today was awesome!   \n",
       "587089   alwyshoutashley  I wish it would stop raining. I'm ready for th...  \n",
       "46597            kasey79  @DannyGirlAlways Ok I still feel kind of bad t...  \n",
       "1409132        Bockman13                   Hanging with Anna and Fernando!   \n",
       "984326        AberdeenUK  Good morning everyone. FYI bought him Rock Ban...  \n",
       "\n",
       "[5000 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downsample the Dataset to 5,000 Total Samples\n",
    "if (dataset==0):\n",
    "    percentage = 0.003125\n",
    "else:\n",
    "    percentage = 0.1\n",
    "df_downsampled = df.sample(frac=percentage,random_state=0)\n",
    "df_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where Sentiment is Positive when Sentiment Value = 1 and Negative when Sentiment Value = 0\n"
     ]
    }
   ],
   "source": [
    "# Visualization of Dataset Sentiment Outcomes - Ensured Even Distribution of Outcomes\n",
    "sent_count = df_downsampled['Sentiment'].value_counts()\n",
    "plt.figure(figsize=(8, 5))\n",
    "w = 0.35  \n",
    "plt.bar(x=np.arange(len(sent_count)), height=sent_count, width = w)\n",
    "plt.xticks(np.arange(len(sent_count)), sent_count.index.tolist())\n",
    "\n",
    "if dataset==0:\n",
    "    plt.xlabel('Tweet Sentiment Value')\n",
    "    plt.ylabel('Tweet Sentiment Value Count')\n",
    "    plt.title('Sentiment140 Twitter Dataset Sentiment Value Frequency')\n",
    "else:\n",
    "    plt.xlabel('Movie Review Sentiment Value')\n",
    "    plt.ylabel('Movie Review Sentiment Value Count')\n",
    "    plt.title('Stanford IMBD Movie Review Dataset Sentiment Value Frequency')\n",
    "plt.show()\n",
    "print('Where Sentiment is Positive when Sentiment Value = 1 and Negative when Sentiment Value = 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beach haha gon miss weekend hopefully something happen again update soon beach i',\n",
       " 'reason i miss nephew lot today sigh',\n",
       " 'i only know bubble co rock i only assume mean i wa bubble',\n",
       " 'i clearly need try bath part i mean too bad i wa nt bootcamp']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Function: Pre-Processes Samples\n",
    "    ## Inputs: samples:        Array of samples\n",
    "    ##         sample_results: Sentiment of Input Samples\n",
    "    ## Outputs: pre_procc_samps: Array of samples pre-processed\n",
    "    ##          pre_procc_res:   Array of results for pre-processed samples      \n",
    "def PreProcess(samples, sample_results):\n",
    "    pre_procc_samps = []\n",
    "    pre_procc_res=[]\n",
    "\n",
    "    # Storing all punctuations using RE library like !;,\"% etc\n",
    "    re_puncs = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # Lemmatizing object\n",
    "    lem = WordNetLemmatizer()\n",
    "\n",
    "    index=0\n",
    "    for sample in samples:\n",
    "        \n",
    "        # Replace Repeated Characters with 2 instance and get rid or URLs / Handles\n",
    "        sample=re.sub(r\"(\\w)\\1{2,}\", r\"\\1\\1\", str(sample))\n",
    "        sample = re.sub(r\"http\\S+\", \"\", str(sample))\n",
    "        sample = re.sub(r\"@\\S+\", \"\", str(sample))\n",
    "\n",
    "        # Get words in sample\n",
    "        words = word_tokenize(str(sample))\n",
    "\n",
    "        # Converting all characters to lower case\n",
    "        words_lower = [w.lower() for w in words]              \n",
    "\n",
    "        # Remove all punctuation\n",
    "        words_lower_no_punc = [re_puncs.sub('', w) for w in words_lower]\n",
    "\n",
    "        # Keep only alpha words\n",
    "        words_lower_alpha = [i for i in words_lower_no_punc if i.isalpha()]\n",
    "\n",
    "        # POS Tagging\n",
    "        pos_tagged_words = nltk.pos_tag(words_lower_alpha)\n",
    "        filtered_pos = [t[0] for t in pos_tagged_words if t[1] == \"NN\" or t[1] == \"NNS\" or t[1].startswith('J') or t[1].startswith('RB') or t[1].startswith('V') or t[1] == \"UH\" or t[1] == \"WRB\" or t[1] == \"POS\"]\n",
    "\n",
    "        # Doing Lemmatizing of words\n",
    "        words_lower_alpha_pos_lemma = [lem.lemmatize(w) for w in filtered_pos]\n",
    "\n",
    "        # Convert back to string and (possibly) one-hot encode tweet\n",
    "        pre_procc_str = ' '.join(words_lower_alpha_pos_lemma)\n",
    "        if (pre_procc_str != \"\"):\n",
    "            pre_procc_samps.append(pre_procc_str)\n",
    "            pre_procc_res.append(sample_results[index])\n",
    "        index=index+1\n",
    "        \n",
    "    return pre_procc_samps, pre_procc_res\n",
    "\n",
    "# Pre-Proccess the Dataset\n",
    "if dataset==0:\n",
    "    Xdf, Ydf = PreProcess(df_downsampled['Tweet'].to_numpy(), df_downsampled['Sentiment'].to_numpy())\n",
    "else:\n",
    "    Xdf, Ydf = PreProcess(df_downsampled['Review'].to_numpy(), df_downsampled['Sentiment'].to_numpy())\n",
    "\n",
    "# Get Final Train/Test Sets:\n",
    "TrainXdf,TestXdf, TrainYdf, TestYdf = train_test_split(Xdf, Ydf,test_size=.2, random_state=2)\n",
    "TrainYdf=np.array(TrainYdf)\n",
    "TestYdf=np.array(TestYdf)\n",
    "TrainXdf[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Training Initialization\n",
    "### Generate Lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function: Automatic Unigram and Bigram Lexicon Generation\n",
    "    ## Inputs: TrainXdf: Training Samples\n",
    "    ##         TrainYdf: Training Sample Results\n",
    "    ## Outputs: unigram_lexicon: Unigram Lexicon generated from Labelled Training Data\n",
    "    ##          bigram_lexicon:  Bigram Lexicon generated from Labelled Training Data\n",
    "def buildLexicons(TrainXdf,TrainYdf):\n",
    "    \n",
    "    # Initialization\n",
    "    unigram_pos_count_dict={}\n",
    "    unigram_neg_count_dict={}\n",
    "    unigram_lexicon={}\n",
    "\n",
    "    bigram_pos_count_dict={}\n",
    "    bigram_neg_count_dict={}\n",
    "    bigram_lexicon={}\n",
    "    \n",
    "    index=0\n",
    "\n",
    "    for tweet in TrainXdf:\n",
    "\n",
    "        # Get words in tweet\n",
    "        words = word_tokenize(str(tweet))\n",
    "\n",
    "        # Initialization\n",
    "        next_word_idx = 1\n",
    "        prev_unigrams=[]\n",
    "        prev_bigrams=[]\n",
    "\n",
    "        for word in words:\n",
    "    \n",
    "            # Unigram lexicon:\n",
    "            # Increment Positive Count\n",
    "            if TrainYdf[index]==1 and word not in prev_unigrams:\n",
    "                if word in unigram_pos_count_dict:\n",
    "                    unigram_pos_count_dict[word]=unigram_pos_count_dict[word]+1\n",
    "                else:\n",
    "                    unigram_pos_count_dict[word]=1\n",
    "                    unigram_neg_count_dict[word]=0\n",
    "                prev_unigrams.append(word)\n",
    "            # Increment Negative Count\n",
    "            elif TrainYdf[index]==0 and word not in prev_unigrams:\n",
    "                if word in unigram_neg_count_dict:\n",
    "                    unigram_neg_count_dict[word]=unigram_neg_count_dict[word]+1\n",
    "                else:\n",
    "                    unigram_neg_count_dict[word]=1\n",
    "                    unigram_pos_count_dict[word]=0\n",
    "                prev_unigrams.append(word)\n",
    "\n",
    "            # Bigram lexicon:\n",
    "            if (next_word_idx < len(words)):\n",
    "                bigram = word + \" \" + words[next_word_idx]\n",
    "                next_word_idx = next_word_idx + 1\n",
    "\n",
    "                # Increment Positive Count\n",
    "                if TrainYdf[index]==1 and bigram not in prev_bigrams:\n",
    "                    if bigram in bigram_pos_count_dict:\n",
    "                        bigram_pos_count_dict[bigram]=bigram_pos_count_dict[bigram]+1\n",
    "                    else:\n",
    "                        bigram_pos_count_dict[bigram]=1\n",
    "                        bigram_neg_count_dict[bigram]=0\n",
    "                    prev_bigrams.append(bigram)\n",
    "                # Increment Negative Count\n",
    "                elif TrainYdf[index]==0 and bigram not in prev_bigrams:\n",
    "                    if bigram in bigram_neg_count_dict:\n",
    "                        bigram_neg_count_dict[bigram]=bigram_neg_count_dict[bigram]+1\n",
    "                    else:\n",
    "                        bigram_neg_count_dict[bigram]=1\n",
    "                        bigram_pos_count_dict[bigram]=0\n",
    "                    prev_bigrams.append(bigram)\n",
    "\n",
    "        index=index+1\n",
    "    \n",
    "    # Calculate polarity score for each word and add pair to the unigram lexicon\n",
    "    for key in unigram_pos_count_dict.keys():\n",
    "        if ((unigram_pos_count_dict[key]+unigram_neg_count_dict[key]) >= .01*len(TrainYdf)):\n",
    "            pos_sent_score = unigram_pos_count_dict[key]/(unigram_pos_count_dict[key]+unigram_neg_count_dict[key])\n",
    "            if (pos_sent_score < 0.4 or pos_sent_score > 0.6):\n",
    "                polarity_score = 2*pos_sent_score-1\n",
    "                unigram_lexicon[key]=polarity_score\n",
    "\n",
    "    # Calculate polarity score for each word and add pair to the bigram lexicon\n",
    "    for key in bigram_pos_count_dict.keys():\n",
    "        if ((bigram_pos_count_dict[key]+bigram_neg_count_dict[key]) >= .01*len(TrainYdf)):\n",
    "            pos_sent_score = bigram_pos_count_dict[key]/(bigram_pos_count_dict[key]+bigram_neg_count_dict[key])\n",
    "            if (pos_sent_score < 0.4 or pos_sent_score > 0.6):\n",
    "                polarity_score = 2*pos_sent_score-1\n",
    "                bigram_lexicon[key]=polarity_score\n",
    "\n",
    "    return unigram_lexicon, bigram_lexicon\n",
    "\n",
    "# Automatic Unigram and Bigram Lexicon Generation\n",
    "auto_unigram_lexicon, auto_bigram_lexicon = buildLexicons(TrainXdf,TrainYdf)\n",
    "\n",
    "# Get WKWSCI Lexicon\n",
    "xls = ExcelFile('WKWSCISentimentLexicon_v1.1.xlsx')\n",
    "data = xls.parse(xls.sheet_names[3])\n",
    "WKWSCI_lexicon = data.set_index(\"term\")[\"sentiment\"].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexicon Value List Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function: Get list of possible lexicon values\n",
    "    ## Inputs: totalLexLevels: Total Number of Lexicon Score Values\n",
    "    ##         minLexVal:      Smallest Lexicon Score Value \n",
    "    ##         maxLexVal:      Largest Lexicon Score Value \n",
    "    ## Output: lexiconValueList: List of All possible Lexicon Values\n",
    "def getLexiconValueList(totalLexLevels, minLexVal, maxLexVal):\n",
    "    lexiconValueList = []\n",
    "    length = maxLexVal - minLexVal\n",
    "    gap = length / totalLexLevels\n",
    "\n",
    "    for val in range(totalLexLevels):\n",
    "        lexiconValueList.append(minLexVal + val*gap)\n",
    "\n",
    "    lexiconValueList.append(maxLexVal)\n",
    "    return lexiconValueList\n",
    "\n",
    "# Parameters:\n",
    "HV_dim = 10000\n",
    "minLexVal = -1\n",
    "maxLexVal = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item Memory Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function: Item Memory Generation\n",
    "    ## Inputs: totalLexLevels: Total Number of Lexicon Score Values\n",
    "    ##         minLexVal:      Smallest Lexicon Score Value \n",
    "    ##         HV_dim:         Dimension of HV\n",
    "    ## Output: itemMem: Item Memory containing HVs for each Possible Lexicon Score Value\n",
    "def itemMemGen(totalLexLevels, minLexVal, HV_dim):\n",
    "    itemMem = dict()\n",
    "    indexVector = range(HV_dim)\n",
    "    nextLevel = int((HV_dim/2/totalLexLevels))\n",
    "    change = int(HV_dim/2)\n",
    "    for level in range(totalLexLevels):\n",
    "        name = level\n",
    "        if(level == 0):\n",
    "            base = np.full(HV_dim, minLexVal)\n",
    "            toOne = np.random.permutation(indexVector)[:change]\n",
    "        else:\n",
    "            toOne = np.random.permutation(indexVector)[:nextLevel]\n",
    "        for index in toOne:\n",
    "            base[index] = base[index] * -1\n",
    "        itemMem[name] = copy.copy(base)\n",
    "    return itemMem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function: Binary search for Value in Lexicon Value List\n",
    "    ## Inputs: value:            Value Trying to Find in List\n",
    "    ##         lexiconValueList: List of Possible Lexicon Values\n",
    "    ## Output: ValueIndex: Index of Desired Value in Lexicon Value List\n",
    "def numToValueIndex(value, lexiconValueList):\n",
    "    if (value == lexiconValueList[-1]):\n",
    "        return len(lexiconValueList)-2\n",
    "    upperIndex = len(lexiconValueList) - 1\n",
    "    lowerIndex = 0\n",
    "    ValueIndex = 0\n",
    "    while (upperIndex > lowerIndex):\n",
    "        ValueIndex = int((upperIndex + lowerIndex)/2)\n",
    "        if (lexiconValueList[ValueIndex] <= value and lexiconValueList[ValueIndex+1] > value):\n",
    "            return ValueIndex\n",
    "        if (lexiconValueList[ValueIndex] > value):\n",
    "            upperIndex = ValueIndex\n",
    "            ValueIndex = int((upperIndex + lowerIndex)/2)\n",
    "        else:\n",
    "            lowerIndex = ValueIndex\n",
    "            ValueIndex = int((upperIndex + lowerIndex)/2)\n",
    "    return ValueIndex\n",
    "\n",
    "### Function: Normalizes WKWSCI Sentiment Score in the Range [-1, 1]\n",
    "    ## Input: value: WKWSCI value\n",
    "    ## Output: norm_val: Normalized WKWSCI value\n",
    "def normWKWSCI(value):\n",
    "    WKWSCI_max = 3\n",
    "    WKWSCI_min = -3\n",
    "    norm_val = 2 * ((value-WKWSCI_min)/(WKWSCI_max-WKWSCI_min))- 1\n",
    "    return norm_val\n",
    "\n",
    "### Function: Encodes a Sample into a HV using LEXICON HDC Approach\n",
    "    ## Inputs: sample:               Training Sample\n",
    "    ##         itemMem:              Generated Item Memory\n",
    "    ##         lexiconValueList:     List of Possible Lexicon Values \n",
    "    ##         encode_method:        Encode Method (0 - ADD, 1 - MULT)\n",
    "    ##         lex_type_flag:        Type of Lexicon Using (0 - UNIGRAM, 1 - BIGRAM)\n",
    "    ##         combo_flag:           Lexicon(s) Using (0 - WKWSCI, 1 - EITHER WKWSCI+AUTO or AUTO [DEPENDS ON LEX_TYPE] )\n",
    "    ##         WKWSCI_lexicon:       WKWSCI Lexicon\n",
    "    ##         auto_unigram_lexicon: Automatic Unigram Lexicon Generated\n",
    "    ##         auto_bigram_lexicon:  Automatic Bigram Lexicon Generated\n",
    "    ## Output: sample_HV: HV of inputted sample \n",
    "def encode(sample, itemMem, lexiconValueList, encode_method, lex_type_flag, combo_flag, WKWSCI_lexicon, auto_unigram_lexicon, auto_bigram_lexicon):\n",
    "    sample_HV = np.zeros(HV_dim, dtype='int32')\n",
    "    sample = word_tokenize(sample)\n",
    "    \n",
    "    next_word_idx=1\n",
    "    first=1\n",
    "\n",
    "    for word in sample:\n",
    "        if combo_flag==1:\n",
    "            if lex_type_flag==0:\n",
    "                if word in WKWSCI_lexicon:\n",
    "                    raw_lexicon_score = WKWSCI_lexicon[word]\n",
    "                    lexicon_score = normWKWSCI(raw_lexicon_score)\n",
    "                    if word in auto_unigram_lexicon:\n",
    "                        other_score = auto_unigram_lexicon[word]\n",
    "                        lexicon_score=0.5*(lexicon_score + other_score)\n",
    "                elif word in auto_unigram_lexicon:\n",
    "                    lexicon_score = auto_unigram_lexicon[word]\n",
    "                else:\n",
    "                    continue\n",
    "            elif next_word_idx < len(sample):\n",
    "                bigram = word + \" \" + sample[next_word_idx]\n",
    "                next_word_idx=next_word_idx+1\n",
    "                if bigram in auto_bigram_lexicon:\n",
    "                    lexicon_score = auto_bigram_lexicon[bigram]\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "        elif combo_flag==0 and lex_type_flag==0:\n",
    "            if word in WKWSCI_lexicon:\n",
    "                raw_lexicon_score = WKWSCI_lexicon[word]\n",
    "                lexicon_score = normWKWSCI(raw_lexicon_score)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        HV_score_index = numToValueIndex(lexicon_score, lexiconValueList)\n",
    "\n",
    "        if encode_method==0:   \n",
    "            if first==1:\n",
    "                sample_HV = itemMem[HV_score_index]\n",
    "                first=0\n",
    "            else:\n",
    "                sample_HV=sample_HV*itemMem[HV_score_index]\n",
    "        else:\n",
    "            sample_HV=sample_HV+itemMem[HV_score_index]\n",
    "    \n",
    "    sample_HV=sample_HV.flatten()\n",
    "\n",
    "    return sample_HV\n",
    "\n",
    "\n",
    "### Function: LEXICON HDC Training Function that creates an Associative Memory for the Model\n",
    "    ## Inputs: X:                    Training Samples\n",
    "    ##         Y:                    Outputs of Training Samples\n",
    "    ##         itemMem:              Generated Item Memory\n",
    "    ##         HV_dim:               Dimension of HV\n",
    "    ##         sent_count:           Number of Possible Sentiment Values\n",
    "    ##         lexiconValueList:     List of Possible Lexicon Values \n",
    "    ##         encode_method:        Encode Method (0 - ADD, 1 - MULT)\n",
    "    ##         lex_type_flag:        Type of Lexicon Using (0 - UNIGRAM, 1 - BIGRAM)\n",
    "    ##         combo_flag:           Lexicon(s) Using (0 - WKWSCI, 1 - EITHER WKWSCI+AUTO or AUTO [DEPENDS ON LEX_TYPE] )\n",
    "    ##         WKWSCI_lexicon:       WKWSCI Lexicon\n",
    "    ##         auto_unigram_lexicon: Automatic Unigram Lexicon Generated\n",
    "    ##         auto_bigram_lexicon:  Automatic Bigram Lexicon Generated\n",
    "    ## Output: assocMem: Associative Memory \n",
    "def train(X, Y, itemMem, HV_dim, sent_count, lexiconValueList, encode_method, lex_type_flag, combo_flag, WKWSCI_lexicon, auto_unigram_lexicon, auto_bigram_lexicon):\n",
    "    assocMem = np.zeros((sent_count, HV_dim), dtype='int32')\n",
    "    sample_idx = 0\n",
    "    \n",
    "    for sample in X:\n",
    "        sample_HV = encode(sample, itemMem, lexiconValueList, encode_method, lex_type_flag, combo_flag, WKWSCI_lexicon, auto_unigram_lexicon, auto_bigram_lexicon)\n",
    "        assocMem[Y[sample_idx]] = np.add(assocMem[Y[sample_idx]], sample_HV)\n",
    "        sample_idx += 1\n",
    "    \n",
    "    return assocMem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function: Compares Input HV to Class HVs and Returns the Predicted Class\n",
    "    ## Inputs: assocMem: Model's Associative Memory\n",
    "    ##         inputHV:  Encoded HV of a sample\n",
    "    ## Output: pred: the predicted class\n",
    "def get_prediction(assocMem, inputHV):\n",
    "    pred = assocMem[0]\n",
    "    maximum = np.NINF\n",
    "\n",
    "    for index in range(len(assocMem)):\n",
    "        similarity = cosine_similarity([inputHV, assocMem[index]])[0][1]  \n",
    "        if (similarity > maximum):\n",
    "            pred = index\n",
    "            maximum = similarity\n",
    "\n",
    "    return pred\n",
    "\n",
    "### Function: Tests the LEXICON HDC Model and Returns Accuracy of Model\n",
    "    ## Inputs: itemMem:          Generated Item Memory\n",
    "    ##         assocMem:         Model's Associative Memory\n",
    "    ##         TestXdf:          Test Samples\n",
    "    ##         TextYdf:          Sentiment of Test Samples\n",
    "    ##         lexiconValueList: List of Possible Lexicon Values \n",
    "    ##         encode_method:    Encode Method (0 - ADD, 1 - MULT)\n",
    "    ##         lex_type_flag:        Type of Lexicon Using (0 - UNIGRAM, 1 - BIGRAM)\n",
    "    ##         combo_flag:           Lexicon(s) Using (0 - WKWSCI, 1 - EITHER WKWSCI+AUTO or AUTO [DEPENDS ON LEX_TYPE] )\n",
    "    ##         WKWSCI_lexicon:   WKWSCI Lexicon\n",
    "    ##         auto_unigram_lexicon: Automatic Unigram Lexicon Generated\n",
    "    ##         auto_bigram_lexicon:  Automatic Bigram Lexicon Generated\n",
    "    ## Output: accuracy: Accuracy of the Model\n",
    "def test(itemMem, assocMem, TestXdf, TestYdf, lexiconValueList, encode_method, lex_type_flag, combo_flag, WKWSCI_lexicon, auto_unigram_lexicon, auto_bigram_lexicon):\n",
    "    true_pos_count=0\n",
    "    false_pos_count=0\n",
    "    correct_count = 0\n",
    "\n",
    "    for index in range(len(TestXdf)):\n",
    "        prediction = get_prediction(assocMem, encode(TestXdf[index], itemMem, lexiconValueList, encode_method, lex_type_flag, combo_flag, WKWSCI_lexicon, auto_unigram_lexicon, auto_bigram_lexicon))\n",
    "        if (TestYdf[index] == prediction):\n",
    "            correct_count += 1\n",
    "            if prediction==1:\n",
    "                true_pos_count = true_pos_count + 1\n",
    "        elif prediction==1:\n",
    "            false_pos_count = false_pos_count + 1\n",
    "    \n",
    "    accuracy = (correct_count / len(TestYdf)) * 100\n",
    "    if (true_pos_count+false_pos_count) != 0:\n",
    "        precision = (true_pos_count/ (true_pos_count+false_pos_count)) * 100\n",
    "    else:\n",
    "        precision=0\n",
    "    return accuracy, precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Hyperparameter Search\n",
    "### One-Shot Training/Accuracy of Various Sets of Hyperparameter Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Various Hyperparameter Sets:\n",
      "LEXICON(S) USED     TYPE OF LEXICON(S)      NUMBER OF LEXICON VALUES    ENCODE METHOD    ONE-SHOT ACCURACY    ONE-SHOT-PRECISION    TRAINING TIME (s)    NUMBER OF TRAINING SAMPLES    TESTING TIME (s)    NUMBER OF TESTING SAMPLES\n",
      "------------------  --------------------  --------------------------  ---------------  -------------------  --------------------  -------------------  ----------------------------  ------------------  ---------------------------\n",
      "WKWSCI              UNIGRAM                                       75                0              57.0712               59.7015             1.06412                           3988            1.6466                            997\n",
      "WKWSCI              UNIGRAM                                       75                1              62.989                68.0288             1.86901                           3988            1.54309                           997\n",
      "WKWSCI              UNIGRAM                                      100                0              54.664                56.2852             1.55584                           3988            1.48403                           997\n",
      "WKWSCI              UNIGRAM                                      100                1              62.989                67.7725             1.44713                           3988            1.00631                           997\n",
      "WKWSCI              UNIGRAM                                      150                0              54.664                56.2852             0.958438                          3988            0.910576                          997\n",
      "WKWSCI              UNIGRAM                                      150                1              62.989                67.7725             0.962421                          3988            0.956445                          997\n",
      "WKWSCI + AUTOMATIC  UNIGRAM                                       75                0              51.0532               53.3477             1.03866                           3988            0.925526                          997\n",
      "WKWSCI + AUTOMATIC  UNIGRAM                                       75                1              68.1043               69.8225             0.954479                          3988            0.971422                          997\n",
      "WKWSCI + AUTOMATIC  UNIGRAM                                      100                0              50.8526               53.1049             0.990386                          3988            0.980371                          997\n",
      "WKWSCI + AUTOMATIC  UNIGRAM                                      100                1              68.4052               70                  1.00257                           3988            0.977355                          997\n",
      "WKWSCI + AUTOMATIC  UNIGRAM                                      150                0              50.5517               52.7542             1.03626                           3988            0.98733                           997\n",
      "WKWSCI + AUTOMATIC  UNIGRAM                                      150                1              68.2046               69.8039             1.03845                           3988            0.985406                          997\n",
      "AUTOMATIC           BIGRAM                                        75                0              50.2508               60.7477             0.631312                          3988            0.840716                          997\n",
      "AUTOMATIC           BIGRAM                                        75                1              50.2508               60.3604             0.644312                          3988            0.841762                          997\n",
      "AUTOMATIC           BIGRAM                                       100                0              50.2508               60.7477             0.645755                          3988            0.829747                          997\n",
      "AUTOMATIC           BIGRAM                                       100                1              50.2508               60.3604             0.637297                          3988            0.850732                          997\n",
      "AUTOMATIC           BIGRAM                                       150                0              50.2508               60.7477             0.643312                          3988            0.840717                          997\n",
      "AUTOMATIC           BIGRAM                                       150                1              50.2508               60.3604             0.618347                          3988            0.834779                          997\n",
      "\n",
      "Encode Method Key: 0 for MULT, 1 for ADD\n",
      "\n",
      "Best Hyperparameters: LEXICON(S) USED:  WKWSCI + AUTOMATIC , TYPE OF LEXICON(S):  UNIGRAM , NUMBER OF LEXICON VALUES:  100 , ENCODE METHOD:  1\n",
      "Best One-Shot Accuracy:  68.40521564694082\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "lex_combos = [0, 1] # 0 for WKWSCI, 1 for WKWSCI+AUTO\n",
    "auto_lexicon_type = [0, 1] # 0 for UNIGRAM, 1 for BIGRAM\n",
    "num_lex_values = [75, 100, 150]\n",
    "encode_method = [0, 1] # 0 for MULT, 1 for ADD \n",
    "\n",
    "# Optimal Result Initialization\n",
    "best_acc=0\n",
    "lex_combos_best=0\n",
    "auto_lexicon_type_best=0\n",
    "num_lex_values_best=0\n",
    "encode_method_best=0\n",
    "output_best_lex_used=\"\"\n",
    "output_best_lex_type=\"\"\n",
    "lexiconValueList_best=[]\n",
    "best_assocMem=[]\n",
    "itemMem_best=[]\n",
    "\n",
    "# Generate Item Memories and Lexicon Value Lists Once for Each Option of Number of Lexicon Values\n",
    "ItemMem_75, ItemMem_100, ItemMem_150=[], [], []\n",
    "LexValueList_75, LexValueList_100, LexValueList_150=[], [], []\n",
    "for num_lex in num_lex_values:\n",
    "    if num_lex==75:\n",
    "        ItemMem_75 = itemMemGen(num_lex, minLexVal, HV_dim)\n",
    "        LexValueList_75 = getLexiconValueList(num_lex, minLexVal, maxLexVal)\n",
    "    elif num_lex==100:\n",
    "        ItemMem_100 = itemMemGen(num_lex, minLexVal, HV_dim)\n",
    "        LexValueList_100 = getLexiconValueList(num_lex, minLexVal, maxLexVal)\n",
    "    elif num_lex==150:\n",
    "        ItemMem_150 = itemMemGen(num_lex, minLexVal, HV_dim)\n",
    "        LexValueList_150 = getLexiconValueList(num_lex, minLexVal, maxLexVal)\n",
    "\n",
    "# Generate Table Initialization\n",
    "table_data=[]\n",
    "col_names = [\"LEXICON(S) USED\", \"TYPE OF LEXICON(S)\", \"NUMBER OF LEXICON VALUES\", \"ENCODE METHOD\", \"ONE-SHOT ACCURACY\", \"ONE-SHOT-PRECISION\", \"TRAINING TIME (s)\", \"NUMBER OF TRAINING SAMPLES\", \"TESTING TIME (s)\", \"NUMBER OF TESTING SAMPLES\"]\n",
    "\n",
    "for auto_lex_type in auto_lexicon_type:\n",
    "    for combo in lex_combos:            \n",
    "        if auto_lex_type==1 and combo==0:\n",
    "            continue\n",
    "\n",
    "        for num_lex in num_lex_values:\n",
    "            # Set ItemMem and Lexicon Value List\n",
    "            if num_lex==75:\n",
    "                itemMem = copy.copy(ItemMem_75)\n",
    "                lexiconValueList = copy.copy(LexValueList_75)\n",
    "            elif num_lex==100:\n",
    "                itemMem = copy.copy(ItemMem_100)\n",
    "                lexiconValueList = copy.copy(LexValueList_100)\n",
    "            elif num_lex==150:\n",
    "                itemMem = copy.copy(ItemMem_150)\n",
    "                lexiconValueList = copy.copy(LexValueList_150)\n",
    "\n",
    "            for method in encode_method:\n",
    "\n",
    "                # Train Model (i.e. Generate Model's Associative Memory)\n",
    "                t0=time.time()\n",
    "                assocMem = train(TrainXdf, TrainYdf, itemMem, HV_dim, len(sent_count), lexiconValueList, method, auto_lex_type, combo, WKWSCI_lexicon, auto_unigram_lexicon, auto_bigram_lexicon)\n",
    "                t1=time.time()\n",
    "                train_time = t1-t0\n",
    "\n",
    "                # One-Shot Training Results\n",
    "                t0=time.time()\n",
    "                one_shot_accuracy, one_shot_precision =test(itemMem, assocMem, TestXdf, TestYdf, lexiconValueList, method, auto_lex_type, combo, WKWSCI_lexicon, auto_unigram_lexicon, auto_bigram_lexicon)\n",
    "                t1=time.time()\n",
    "                test_time = t1-t0\n",
    "\n",
    "                if combo==1:\n",
    "                    if auto_lex_type==0:\n",
    "                        # Collect Data for Table\n",
    "                        data = [\"WKWSCI + AUTOMATIC\", \"UNIGRAM\", num_lex, method, one_shot_accuracy, one_shot_precision, train_time, len(TrainYdf), test_time, len(TestYdf)]\n",
    "                        lex_used=\"WKWSCI + AUTOMATIC\"\n",
    "                        lex_type=\"UNIGRAM\"\n",
    "                    else:\n",
    "                        # Collect Data for Table\n",
    "                        data = [\"AUTOMATIC\", \"BIGRAM\", num_lex, method, one_shot_accuracy, one_shot_precision, train_time, len(TrainYdf), test_time, len(TestYdf)]\n",
    "                        lex_used=\"AUTOMATIC\"\n",
    "                        lex_type=\"BIGRAM\"\n",
    "                elif combo==0:\n",
    "                    # Collect Data for Table\n",
    "                    data = [\"WKWSCI\", \"UNIGRAM\", num_lex, method, one_shot_accuracy, one_shot_precision, train_time, len(TrainYdf), test_time, len(TestYdf)]\n",
    "                    lex_used=\"WKWSCI\"\n",
    "                    lex_type=\"UNIGRAM\"\n",
    "\n",
    "                # Add Data to Table\n",
    "                table_data.append(data)\n",
    "\n",
    "                if one_shot_accuracy>best_acc:\n",
    "                    best_acc=one_shot_accuracy\n",
    "                    best_assocMem=copy.copy(assocMem)\n",
    "                    itemMem_best=copy.copy(itemMem)\n",
    "                    lexiconValueList_best=copy.copy(lexiconValueList)\n",
    "                    \n",
    "                    lex_combos_best=combo\n",
    "                    auto_lexicon_type_best=auto_lex_type\n",
    "                    num_lex_values_best=num_lex\n",
    "                    encode_method_best=method\n",
    "\n",
    "                    output_best_lex_used=lex_used\n",
    "                    output_best_lex_type=lex_type\n",
    "\n",
    "# Get Necessary Components for Best Model\n",
    "assocMem=copy.copy(best_assocMem)\n",
    "itemMem=copy.copy(itemMem_best)\n",
    "lexiconValueList=copy.copy(lexiconValueList_best)\n",
    "encode_method=encode_method_best\n",
    "lexicon_type=auto_lexicon_type_best\n",
    "combo=lex_combos_best\n",
    "\n",
    "# Save Results to File\n",
    "df=pd.DataFrame(table_data, columns=col_names)\n",
    "filepath=\"./Results/HyperparameterResults/LEXICON_\" + str(dataset) +\".csv\"\n",
    "df.to_csv(filepath)\n",
    "            \n",
    "print(\"Results of Various Hyperparameter Sets:\")\n",
    "print(tabulate(table_data, headers=col_names, tablefmt=\"simple\"))\n",
    "print(\"\\nEncode Method Key: 0 for MULT, 1 for ADD\\n\")\n",
    "print(\"Best Hyperparameters: LEXICON(S) USED: \", output_best_lex_used, \", TYPE OF LEXICON(S): \", output_best_lex_type, \", NUMBER OF LEXICON VALUES: \", num_lex_values_best, \", ENCODE METHOD: \", encode_method_best)\n",
    "print(\"Best One-Shot Accuracy: \", best_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retraining Optimal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Retraining Model w Learning Parameter:  20  Epochs --------\n",
      "Epoch  1 :  66.29889669007021\n",
      "Epoch  2 :  68.00401203610834\n",
      "Epoch  3 :  68.90672016048146\n",
      "Epoch  4 :  68.7061183550652\n",
      "Epoch  5 :  65.79739217652958\n",
      "Epoch  6 :  65.89769307923771\n",
      "Epoch  7 :  68.3049147442327\n",
      "Epoch  8 :  68.40521564694082\n",
      "Epoch  9 :  68.00401203610834\n",
      "Epoch  10 :  67.80341023069208\n",
      "Epoch  11 :  68.90672016048146\n",
      "Epoch  12 :  68.40521564694082\n",
      "Epoch  13 :  68.10431293881645\n",
      "Epoch  14 :  68.00401203610834\n",
      "Epoch  15 :  69.00702106318957\n",
      "Epoch  16 :  67.90371113340021\n",
      "Epoch  17 :  69.30792377131394\n",
      "Epoch  18 :  67.70310932798395\n",
      "Epoch  19 :  68.80641925777333\n",
      "Epoch  20 :  68.3049147442327\n",
      "-------- Retraining Model without Learning Parameter:  20  Epochs --------\n",
      "Epoch  1 :  67.70310932798395\n",
      "Epoch  2 :  67.10130391173522\n",
      "Epoch  3 :  67.70310932798395\n",
      "Epoch  4 :  68.10431293881645\n",
      "Epoch  5 :  69.30792377131394\n",
      "Epoch  6 :  68.3049147442327\n",
      "Epoch  7 :  67.70310932798395\n",
      "Epoch  8 :  66.90070210631896\n",
      "Epoch  9 :  69.20762286860582\n",
      "Epoch  10 :  68.60581745235707\n",
      "Epoch  11 :  68.20461384152458\n",
      "Epoch  12 :  68.80641925777333\n",
      "Epoch  13 :  68.60581745235707\n",
      "Epoch  14 :  68.80641925777333\n",
      "Epoch  15 :  67.30190571715146\n",
      "Epoch  16 :  65.79739217652958\n",
      "Epoch  17 :  67.60280842527582\n",
      "Epoch  18 :  66.39919759277834\n",
      "Epoch  19 :  68.7061183550652\n",
      "Epoch  20 :  68.90672016048146\n"
     ]
    }
   ],
   "source": [
    "### Function: VECTORIZED HDC Re-Training Function that creates a New Associative Memory for the Model\n",
    "    ## Inputs: X:                    Training Samples\n",
    "    ##         Y:                    Outputs of Training Samples\n",
    "    ##         itemMem:              Generated Item Memory\n",
    "    ##         assocMem:             Associative Memory of Current Model\n",
    "    ##         lexiconValueList:     List of Possible Lexicon Values \n",
    "    ##         encode_method:        Encode Method (0 - ADD, 1 - MULT)\n",
    "    ##         lex_type_flag:        Type of Lexicon Using (0 - UNIGRAM, 1 - BIGRAM)\n",
    "    ##         combo_flag:           Lexicon(s) Using (0 - WKWSCI, 1 - EITHER WKWSCI+AUTO or AUTO [DEPENDS ON LEX_TYPE] )\n",
    "    ##         WKWSCI_lexicon:       WKWSCI Lexicon\n",
    "    ##         auto_unigram_lexicon: Automatic Unigram Lexicon Generated\n",
    "    ##         auto_bigram_lexicon:  Automatic Bigram Lexicon Generated\n",
    "    ##         alpha:                Learning Rate Parameter\n",
    "    ## Output: assocMem: New Associative Memory\n",
    "def retrain(X, Y, itemMem, assocMem, lexiconValueList, encode_method, lex_type_flag, combo_flag, WKWSCI_lexicon, auto_unigram_lexicon, auto_bigram_lexicon, alpha):\n",
    "    sample_index = 0\n",
    "    for sample in X:\n",
    "        sample_HV = encode(sample, itemMem, lexiconValueList, encode_method, lex_type_flag, combo_flag, WKWSCI_lexicon, auto_unigram_lexicon, auto_bigram_lexicon)\n",
    "        prediction = get_prediction(assocMem, sample_HV)\n",
    "        if prediction != Y[sample_index]:\n",
    "            assocMem[Y[sample_index]] = np.add(assocMem[Y[sample_index]], alpha * sample_HV)\n",
    "            assocMem[prediction] = np.subtract(assocMem[prediction], alpha * sample_HV)\n",
    "        sample_index += 1\n",
    "    return assocMem\n",
    "\n",
    "# Re-Train Optimal Model with Learning Parameter\n",
    "learningparam_results=[]\n",
    "num_epochs = 20\n",
    "print('-------- Retraining Model w Learning Parameter: ', num_epochs, ' Epochs --------')\n",
    "for epoch in range(num_epochs):\n",
    "    assocMem = retrain(TrainXdf, TrainYdf, itemMem, assocMem, lexiconValueList, encode_method, lexicon_type, combo, WKWSCI_lexicon, auto_unigram_lexicon, auto_bigram_lexicon, alpha = num_epochs - epoch)\n",
    "    acc, prec = test(itemMem, assocMem, TestXdf, TestYdf, lexiconValueList, encode_method, lexicon_type, combo, WKWSCI_lexicon, auto_unigram_lexicon, auto_bigram_lexicon)\n",
    "    print('Epoch ', (epoch+1), ': ', acc)\n",
    "    learningparam_results.append([acc,prec])\n",
    "\n",
    "# Re-Train Optimal Model without Learning Parameter\n",
    "assocMem=copy.copy(best_assocMem)\n",
    "no_learningparam_results=[]\n",
    "print('-------- Retraining Model without Learning Parameter: ', num_epochs, ' Epochs --------')\n",
    "for epoch in range(num_epochs):\n",
    "    assocMem = retrain(TrainXdf, TrainYdf, itemMem, assocMem, lexiconValueList, encode_method, lexicon_type, combo, WKWSCI_lexicon, auto_unigram_lexicon, auto_bigram_lexicon, alpha = 1)\n",
    "    acc, prec = test(itemMem, assocMem, TestXdf, TestYdf, lexiconValueList, encode_method, lexicon_type, combo, WKWSCI_lexicon, auto_unigram_lexicon, auto_bigram_lexicon)\n",
    "    print('Epoch ', (epoch+1), ': ', acc)\n",
    "    no_learningparam_results.append([acc,prec])\n",
    "\n",
    "# Save All Results to Files\n",
    "col_name=[\"Accuracy\", \"Precision\"]\n",
    "\n",
    "df_lp=pd.DataFrame(learningparam_results, columns=col_name)\n",
    "filepath=\"./Results/EpochResults_LearningParam/LEXICON_\" + str(dataset) + \".csv\"\n",
    "df_lp.to_csv(filepath)\n",
    "\n",
    "df_nlp=pd.DataFrame(no_learningparam_results, columns=col_name)\n",
    "filepath=\"./Results/EpochResults_NoLearningParam/LEXICON_\" + str(dataset) + \".csv\"\n",
    "df_nlp.to_csv(filepath)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "071eaeccc96c6410cecdb330bf8e8ae0267d24b86e05481c728d399cbe7cbc33"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('aml': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
