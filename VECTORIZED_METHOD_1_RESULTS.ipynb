{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VECTORIZED HDC METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Basic Packages\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from nltk import sent_tokenize,word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from tabulate import tabulate\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from numpy import loadtxt\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Insights and Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Stanford IMBD Movie Review Dataset...\n",
      "Total number of Samples In Dataset: 50000\n"
     ]
    }
   ],
   "source": [
    "### Function: Collects sample data from files in Stanford Dataset Subfolders\n",
    "    ## Inputs: folderpath: Path to Desired Folder\n",
    "    ##         sentiment:  Sentiment Value (0 or 1)\n",
    "    ## Output: df: Pandas Dataframe of all Sample Data found in desired folder\n",
    "def stanfordDatasetFolderDataLoader(folderpath, sentiment):\n",
    "    file_list=listdir(folderpath)\n",
    "\n",
    "    df = pd.DataFrame(columns = ['Review', 'Sentiment'])\n",
    "    for file in file_list:\n",
    "        filepath=folderpath + file\n",
    "        f = open(filepath,'r', encoding=\"utf-8\")\n",
    "        sample = f.read()\n",
    "        f.close()\n",
    "        df = df.append({'Review' : sample, 'Sentiment' : sentiment}, ignore_index = True)\n",
    "    return df\n",
    "\n",
    "# Choose Dataset (0: Sentiment140, 1: Stanford IMBD Dataset)\n",
    "dataset = 1\n",
    "\n",
    "# Load Dataset\n",
    "if (dataset==0):\n",
    "    print('Using Sentiment140 Twitter Dataset...')\n",
    "    # Read in Sentiment140 data from CSV\n",
    "    df = pd.read_csv('./Sentiment140_Tweets/data.csv')\n",
    "    df.columns =['Sentiment', 'IDs', 'Date', 'Flag', 'User', 'Tweet']\n",
    "else:\n",
    "    print('Using Stanford IMBD Movie Review Dataset...')\n",
    "    # Read in Training Stanford IMBD Movie Review data from subfolders\n",
    "    train_pos=stanfordDatasetFolderDataLoader('./StanfordMovie/train/pos/',1)\n",
    "    train_neg=stanfordDatasetFolderDataLoader('./StanfordMovie/train/neg/',0)\n",
    "    train_df=pd.concat([train_pos, train_neg], axis=0)\n",
    "    \n",
    "    # Read in Testing Stanford IMBD Movie Review data from subfolders\n",
    "    test_pos=stanfordDatasetFolderDataLoader('./StanfordMovie/test/pos/',1)\n",
    "    test_neg=stanfordDatasetFolderDataLoader('./StanfordMovie/test/neg/',0)\n",
    "    test_df=pd.concat([test_pos, test_neg], axis=0)\n",
    "\n",
    "    df=pd.concat([train_df, test_df], axis=0)\n",
    "print('Total number of Samples In Dataset:', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset in Use has No NULL values.\n",
      "Dataset Length after Cleanup: 50000\n"
     ]
    }
   ],
   "source": [
    "# Dataset Cleanup:\n",
    "\n",
    "# Sentiment140 Sentiment Clean Up\n",
    "if dataset==0:\n",
    "    # Replace Sentiment of 4 (Positive) with 1\n",
    "    df[\"Sentiment\"].replace({4: 1}, inplace=True)\n",
    "    # Eliminate Neutral Tweets, if any\n",
    "    df = df[df['Sentiment'] != 2]\n",
    "\n",
    "# Check for Null Values\n",
    "if ( not df.isnull().values.any() ):\n",
    "    print(\"Dataset in Use has No NULL values.\")\n",
    "else:\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "print(\"Dataset Length after Cleanup:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11841</th>\n",
       "      <td>Often tagged as a comedy, The Man In The White...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7102</th>\n",
       "      <td>After Chaplin made one of his best films: Doug...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8019</th>\n",
       "      <td>I think the movie was one sided I watched it r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>I have fond memories of watching this visually...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5142</th>\n",
       "      <td>This episode had potential. The basic premise ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9869</th>\n",
       "      <td>Reading the various external reviews of Roger ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4692</th>\n",
       "      <td>To soccer fans every where -- stay away from t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12143</th>\n",
       "      <td>This is such a great movie to watch with young...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9963</th>\n",
       "      <td>have just got back from seeing this brilliantl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10833</th>\n",
       "      <td>This film aka \"the four hundred blows\" is a mi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Review Sentiment\n",
       "11841  Often tagged as a comedy, The Man In The White...         1\n",
       "7102   After Chaplin made one of his best films: Doug...         0\n",
       "8019   I think the movie was one sided I watched it r...         0\n",
       "747    I have fond memories of watching this visually...         1\n",
       "5142   This episode had potential. The basic premise ...         0\n",
       "...                                                  ...       ...\n",
       "9869   Reading the various external reviews of Roger ...         1\n",
       "4692   To soccer fans every where -- stay away from t...         0\n",
       "12143  This is such a great movie to watch with young...         1\n",
       "9963   have just got back from seeing this brilliantl...         1\n",
       "10833  This film aka \"the four hundred blows\" is a mi...         0\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downsample the Dataset to 5000 Total Samples\n",
    "percentage=5000/len(df)\n",
    "df_downsampled = df.sample(frac=percentage,random_state=0)\n",
    "df_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where Sentiment is Positive when Sentiment Value = 1 and Negative when Sentiment Value = 0\n"
     ]
    }
   ],
   "source": [
    "# Visualization of Dataset Sentiment Outcomes - Ensured Even Distribution of Outcomes\n",
    "sent_count = df_downsampled['Sentiment'].value_counts()\n",
    "plt.figure(figsize=(8, 5))\n",
    "w = 0.35  \n",
    "plt.bar(x=np.arange(len(sent_count)), height=sent_count, width = w)\n",
    "plt.xticks(np.arange(len(sent_count)), sent_count.index.tolist())\n",
    "\n",
    "if dataset==0:\n",
    "    plt.xlabel('Tweet Sentiment Value')\n",
    "    plt.ylabel('Tweet Sentiment Value Count')\n",
    "    plt.title('Sentiment140 Twitter Dataset Sentiment Value Frequency')\n",
    "else:\n",
    "    plt.xlabel('Movie Review Sentiment Value')\n",
    "    plt.ylabel('Movie Review Sentiment Value Count')\n",
    "    plt.title('Stanford IMBD Movie Review Dataset Sentiment Value Frequency')\n",
    "plt.show()\n",
    "print('Where Sentiment is Positive when Sentiment Value = 1 and Negative when Sentiment Value = 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like standard sli flick involv top action unbeliev stunt unbeliev not_intend complimentari retard dialogu love steam pile mountain goat dung high hope base trailer thought stalon go forc hasbeen day yield smarter peopl make action film would place credibl hero credibl situat stori set believ action would prevail crave action least close enough realiti imagin fear excit would come event limit knowledg hypothermia effect render least one scene laughabl ridicul judg dredd better know go theater go see comic book made movi charact set everyth els beyond comparison anyth might encount cliffhang hand turn mountain climb guid rambo say yo adrian',\n",
       " 'outing certainli nt_live predecessor share memor moment person favorit lay wast citi block videodisc cannon see close nimoy face singl tear shed left eye know point nimoy kill machin viewer ca nt_help pull emot turmoil understand previous flat affect facad absolut brillianc sex scene display nice balanc carnal not_pornograph afterward felt pretti good understand work magnavis videodisc player bad nt_produc year',\n",
       " 'bad sequel one real one first movi bad fool make sequel wors actor wors scenario wors special effect wors movi histori bad histori give half laugh',\n",
       " 'part lethal dull ventur naach dancerchoreograph antara mali regard kind auteur beyond petti commerci compromis peopl around includ actor beau abhishek bachchan build career nice idea turn howlari bad concept costum danc movement stuff rotten enough make even forc behind ultracheap south indian potboil squirm sever embarrass br br stori follow yawninduc predict pattern dancer beau meet affair ye sex includ beau get success spurn attempt help beau throw attitud walk dancer meet unlik director want want incongru effort bag even incongru popular beau attempt reach rebuf climax burst ca nt_live without_h duh end br br actual nt_realli expect film much good watch antara mali fetish actress imo suffici talent reach grand height provid opportun ala role film look far better paper actual execut iron film suppos uncompromis charact give credit put game effort shine well scene actual ask actress wish movi worthi abhishek bachchan also throw good punch last scene salvag somewhat simmer turn pleasantli remind father br br recommend sit humong pile dogcrap return grain good moment courtesi lead actor']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Function: Pre-Processes Samples\n",
    "    ## Inputs: samples:        Array of samples\n",
    "    ##         sample_results: Sentiment of Input Samples\n",
    "    ## Outputs: pre_procc_samps: Array of samples pre-processed\n",
    "    ##          pre_procc_res:   Array of results for pre-processed samples      \n",
    "def PreProcess(samples, sample_results):\n",
    "    pre_procc_samps = []\n",
    "    pre_procc_res=[]\n",
    "\n",
    "    # Get Negation Words From Text File\n",
    "    negation_word_list = loadtxt(\"negation_words.txt\", delimiter=\"\\n\", dtype=\"str\")\n",
    "    # Storing all punctuations using RE library like !;,\"% etc\n",
    "    re_puncs = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # Storing all stop words like a, an, the, when, there, this etc\n",
    "    stop_word  = set(stopwords.words('english'))\n",
    "    # Lemmatizing object\n",
    "    lem = WordNetLemmatizer()\n",
    "    # Using Porter Stemmer\n",
    "    p_stem = PorterStemmer()\n",
    "\n",
    "    index=0\n",
    "    for sample in samples:\n",
    "        \n",
    "        # Replace Repeated Characters with 2 instance and get rid or URLs / Handles\n",
    "        sample = re.sub(r\"(\\w)\\1{2,}\", r\"\\1\\1\", str(sample))\n",
    "        sample = re.sub(r\"http\\S+\", \"\", str(sample))\n",
    "        sample = re.sub(r\"@\\S+\", \"\", str(sample))\n",
    "\n",
    "        # Get words in sample\n",
    "        words = word_tokenize(str(sample))\n",
    "\n",
    "        # Converting all characters to lower case\n",
    "        words_lower = [w.lower() for w in words]              \n",
    "\n",
    "        # Remove all punctuation\n",
    "        words_lower_no_punc = [re_puncs.sub('', w) for w in words_lower]\n",
    "\n",
    "        # Keep only alpha words\n",
    "        words_lower_alpha = [i for i in words_lower_no_punc if i.isalpha()]\n",
    "\n",
    "        # Negation Handling\n",
    "        handled_words=[]\n",
    "        negation_word=\"\"\n",
    "        for word in words_lower_alpha:\n",
    "            if word in negation_word_list:\n",
    "                negation_word = word\n",
    "            elif negation_word != \"\":\n",
    "                word=negation_word+\"_\"+word\n",
    "                negation_word=\"\"\n",
    "                handled_words.append(word)\n",
    "            else:\n",
    "                handled_words.append(word)\n",
    "\n",
    "        # Removing all stop words\n",
    "        words_lower_alpha_nostop = [w for w in handled_words if w not in stop_word]\n",
    "\n",
    "        # Doing Lemmatizing of words\n",
    "        words_lower_alpha_nostop_lemma = [lem.lemmatize(w) for w in words_lower_alpha_nostop]\n",
    "\n",
    "        # Stemming process\n",
    "        words_lower_alpha_nostop_lemma_stem = [p_stem.stem(w) for w in words_lower_alpha_nostop_lemma]\n",
    "\n",
    "        # Convert back to string and (possibly) one-hot encode tweet\n",
    "        pre_procc_str = ' '.join(words_lower_alpha_nostop_lemma_stem)\n",
    "        if (pre_procc_str != \"\"):\n",
    "            pre_procc_samps.append(pre_procc_str)\n",
    "            pre_procc_res.append(sample_results[index])\n",
    "        index=index+1\n",
    "        \n",
    "    return pre_procc_samps, pre_procc_res\n",
    "\n",
    "# Pre-Proccess the Dataset and Get Final Train/Test Sets:\n",
    "if dataset==0:\n",
    "    Xdf, Ydf = PreProcess(df_downsampled['Tweet'].to_numpy(), df_downsampled['Sentiment'].to_numpy())\n",
    "else:\n",
    "    Xdf, Ydf = PreProcess(df_downsampled['Review'].to_numpy(), df_downsampled['Sentiment'].to_numpy())\n",
    "\n",
    "TrainXdf,TestXdf, TrainYdf, TestYdf = train_test_split(Xdf, Ydf, test_size=.2, random_state=2)\n",
    "\n",
    "TrainYdf=np.array(TrainYdf)\n",
    "TestYdf=np.array(TestYdf)\n",
    "TrainXdf[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Training Initialization\n",
    "### Item Memory Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function: Item Memory Generation\n",
    "    ## Inputs: dim: Number of columns (i.e. length of HV)\n",
    "    ##         num_rows: Number of rows (i.e. number of features)\n",
    "    ## Output: dictMem: Item Memory containing HVs for each supported char\n",
    "def itemMemGen(dim, num_rows):\n",
    "    dictMem = np.random.randint(2, size=(num_rows, dim), dtype='int32')\n",
    "    # dictMem[dictMem == 0] = -1\n",
    "    return dictMem\n",
    "\n",
    "# Parameters:\n",
    "HV_dim = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function: Encodes a Sample into a HV using VECTORIZED HDC Approach\n",
    "    ## Inputs: sample:      Training Sample\n",
    "    ##         itemMem:     Generate Item Memory\n",
    "    ## Output: sample_HV: HV of inputted sample \n",
    "def encode(sample, itemMem):\n",
    "    return np.dot(sample,itemMem)\n",
    "\n",
    "### Function: VECTORIZED HDC Training Function that creates an Associative Memory for the Model\n",
    "    ## Inputs: X:           Training Samples\n",
    "    ##         Y:           Outputs of Training Samples\n",
    "    ##         itemMem:     Generated Item Memory\n",
    "    ##         HV_dim:      Dimension of HV\n",
    "    ##         sent_count:  Number of Possible Sentiment Values\n",
    "    ## Output: assocMem: Associative Memory \n",
    "def train(X, Y, itemMem, HV_dim, sent_count):\n",
    "    assocMem = np.zeros((sent_count, HV_dim), dtype='int32')\n",
    "    sample_idx = 0\n",
    "    \n",
    "    for sample in X:\n",
    "        sample_HV = encode(sample.reshape(1,-1), itemMem)\n",
    "        assocMem[Y[sample_idx]] = np.add(assocMem[Y[sample_idx]], sample_HV)\n",
    "        sample_idx += 1\n",
    "    return assocMem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function: Compares Input HV to Class HVs and Returns the Predicted Class\n",
    "    ## Inputs: assocMem: Model's Associative Memory\n",
    "    ##         inputHV:  Encoded HV of a sample\n",
    "    ## Output: pred: the predicted class\n",
    "def get_prediction(assocMem, inputHV):\n",
    "    pred = assocMem[0]\n",
    "    maximum = np.NINF\n",
    "\n",
    "    for index in range(len(assocMem)):\n",
    "        similarity = cosine_similarity([inputHV, assocMem[index]])[0][1]  \n",
    "        if (similarity > maximum):\n",
    "            pred = index\n",
    "            maximum = similarity\n",
    "\n",
    "    return pred\n",
    "\n",
    "### Function: Tests the VECTORIZED HDC Model and Returns Accuracy of Model\n",
    "    ## Inputs: itemMem:     Generated Item Memory\n",
    "    ##         assocMem:    Model's Associative Memory\n",
    "    ##         TestXdf:     Test Samples\n",
    "    ##         TextYdf:     Sentiment of Test Samples\n",
    "    ## Output: accuracy: Accuracy of the Model\n",
    "def test(itemMem, assocMem, TestXdf, TestYdf):\n",
    "    true_pos_count=0\n",
    "    false_pos_count=0\n",
    "    correct_count = 0\n",
    "\n",
    "    for index in range(len(TestXdf)):\n",
    "        prediction = get_prediction(assocMem, encode(TestXdf[index], itemMem))\n",
    "        if (TestYdf[index] == prediction):\n",
    "            correct_count += 1\n",
    "            if prediction==1:\n",
    "                true_pos_count += 1\n",
    "        elif prediction==1:\n",
    "            false_pos_count += 1\n",
    "            \n",
    "    accuracy = (correct_count / len(TestYdf)) * 100\n",
    "    if (true_pos_count+false_pos_count) != 0:\n",
    "        precision = (true_pos_count/ (true_pos_count+false_pos_count)) * 100\n",
    "    else:\n",
    "        precision=0\n",
    "    return accuracy, precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Hyperparameter Search\n",
    "### One-Shot Training/Accuracy of Various Sets of Hyperparameter Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "711370c8eda44b94aa2043b0a2f1a5b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dad9f3d355d4e06b02cdfcb73196e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b34d7a7d344331842af958c2005687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60525cc7936a4ff8b37e5dd3391e3807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Various Hyperparameter Sets:\n",
      "  VECTOR TYPE  NGRAM      MINIMUM DOCUMENT FREQUENCY    K PERCENT    FINAL NUMBER OF FEATURES    ONE-SHOT ACCURACY    ONE-SHOT-PRECISION    TRAINING TIME (s)    NUMBER OF TRAINING SAMPLES    TESTING TIME (s)    NUMBER OF TESTING SAMPLES\n",
      "-------------  -------  ----------------------------  -----------  --------------------------  -------------------  --------------------  -------------------  ----------------------------  ------------------  ---------------------------\n",
      "            0  (1, 1)                             10         0.35                        1598                 64.2               62.5498            1287.8                             4000           302.545                           1000\n",
      "            0  (1, 1)                             10         0.5                         2284                 64.1               62.475             1906.37                            4000           443.261                           1000\n",
      "            0  (1, 1)                             10         0.75                        3426                 64.4               62.6984            2976                               4000           668.768                           1000\n",
      "            0  (1, 1)                             35         0.35                         631                 65.9               64.503              224.729                           4000            60.8061                          1000\n",
      "            0  (1, 1)                             35         0.5                          901                 65.4               63.8                528.251                           4000           124.88                            1000\n",
      "            0  (1, 1)                             35         0.75                        1352                 65.6               63.8889            1059.06                            4000           249.88                            1000\n",
      "            0  (1, 1)                             60         0.35                         400                 66.2               64.959              100.289                           4000            27.1331                          1000\n",
      "            0  (1, 1)                             60         0.5                          572                 66.6               64.7638             185.976                           4000            50.1586                          1000\n",
      "            0  (1, 1)                             60         0.75                         858                 65.6               63.8889             473.131                           4000           117.369                           1000\n",
      "            0  (2, 2)                             10         0.35                         786                 53.4               52.1845             387.681                           4000            97.7987                          1000\n",
      "            0  (2, 2)                             10         0.5                         1123                 53.5               52.3002             810.657                           4000           193.555                           1000\n",
      "            0  (2, 2)                             10         0.75                        1685                 54.5               53.528             1375.87                            4000           320.588                           1000\n",
      "            0  (2, 2)                             35         0.75                         179                 54.4               53.5                 41.5573                          4000            11.4985                          1000\n",
      "            0  (2, 2)                             35         0.85                         203                 54.5               53.6709              47.3071                          4000            13.3148                          1000\n",
      "            0  (2, 2)                             35         0.9                          215                 54.8               54.0404              50.0272                          4000            14.063                           1000\n",
      "            0  (3, 3)                             10         0.75                         228                 55.2               57.8947              50.1328                          4000            14.5484                          1000\n",
      "            0  (3, 3)                             10         0.85                         259                 54.7               56.3786              61.507                           4000            19.6889                          1000\n",
      "            0  (3, 3)                             10         0.9                          274                 55.4               57.7236              65.5064                          4000            18.0235                          1000\n",
      "            0  (1, 2)                             10         0.35                        2385                 65.1               63.5271            1996.68                            4000           471.402                           1000\n",
      "            0  (1, 2)                             10         0.5                         3407                 66.5               64.9899            2953.91                            4000           670.051                           1000\n",
      "            0  (1, 2)                             10         0.75                        5111                 66.4               65.102             5201.43                            4000          1063.07                            1000\n",
      "            0  (1, 2)                             35         0.35                         714                 67.1               65.5311             294.727                           4000            77.3506                          1000\n",
      "            0  (1, 2)                             35         0.5                         1021                 66.5               64.6943             674.045                           4000           158.969                           1000\n",
      "            0  (1, 2)                             35         0.75                        1531                 66.3               64.6707            1231.74                            4000           299.112                           1000\n",
      "            0  (1, 2)                             60         0.35                         428                 67                 65.4618             110.343                           4000            29.0982                          1000\n",
      "            0  (1, 2)                             60         0.5                          611                 67.1               65.4691             216.041                           4000            63.374                           1000\n",
      "            0  (1, 2)                             60         0.75                         917                 66                 64.4578             559.536                           4000           128.096                           1000\n",
      "            0  (1, 3)                             10         0.35                        2492                 65.8               64.257             2075.64                            4000           481.389                           1000\n",
      "            0  (1, 3)                             10         0.5                         3560                 66.1               64.5875            3172.2                             4000           703.906                           1000\n",
      "            0  (1, 3)                             10         0.75                        5340                 65.6               64.1129            5886.99                            4000          1141.88                            1000\n",
      "            0  (1, 3)                             35         0.35                         726                 67.5               66.2577             307.877                           4000            82.2202                          1000\n",
      "            0  (1, 3)                             35         0.5                         1038                 66.9               65.1485             692.244                           4000           163.094                           1000\n",
      "            0  (1, 3)                             35         0.75                        1557                 65.6               64.0562            1267.6                             4000           300.952                           1000\n",
      "            0  (1, 3)                             60         0.35                         432                 67.2               65.5378             112.147                           4000            29.2326                          1000\n",
      "            0  (1, 3)                             60         0.5                          618                 66                 64.4578             218.556                           4000            59.0277                          1000\n",
      "            0  (1, 3)                             60         0.75                         927                 66.1               64.5291             561.862                           4000           134.11                            1000\n",
      "            1  (1, 1)                             10         0.35                        1598                 80.5               79.7938            1300.69                            4000           307.527                           1000\n",
      "            1  (1, 1)                             10         0.5                         2284                 79.7               78.3838            1894.48                            4000           445.746                           1000\n",
      "            1  (1, 1)                             10         0.75                        3426                 79.5               78.2961            2989.5                             4000           681.23                            1000\n",
      "            1  (1, 1)                             35         0.35                         631                 79.3               78.4394             221.228                           4000            58.9514                          1000\n",
      "            1  (1, 1)                             35         0.5                          901                 79.9               78.1312             520.497                           4000           126.764                           1000\n",
      "            1  (1, 1)                             35         0.75                        1352                 80.6               79.3522            1060.38                            4000           251.823                           1000\n",
      "            1  (1, 1)                             60         0.35                         400                 79.5               78.1818             100.399                           4000            26.6306                          1000\n",
      "            1  (1, 1)                             60         0.5                          572                 80.6               79                  183.15                            4000            48.7563                          1000\n",
      "            1  (1, 1)                             60         0.75                         858                 80.3               78.8732             474.143                           4000           115.039                           1000\n",
      "            1  (2, 2)                             10         0.35                         786                 73.1               73.6264             390.297                           4000            96.477                           1000\n",
      "            1  (2, 2)                             10         0.5                         1123                 65.9               64.6217             813.72                            4000           194.858                           1000\n",
      "            1  (2, 2)                             10         0.75                        1685                 68.4               66.3424            1390.02                            4000           325.98                            1000\n",
      "            1  (2, 2)                             35         0.75                         179                 60.7               59.4203              40.9717                          4000            11.4614                          1000\n",
      "            1  (2, 2)                             35         0.85                         203                 63.3               61.9145              48.0339                          4000            13.4164                          1000\n",
      "            1  (2, 2)                             35         0.9                          215                 62.9               61.3682              50.4447                          4000            14.137                           1000\n",
      "            1  (3, 3)                             10         0.75                         228                 55.1               57.6419              54.1983                          4000            15.1516                          1000\n",
      "            1  (3, 3)                             10         0.85                         259                 54.7               56.1265              61.661                           4000            17.3502                          1000\n",
      "            1  (3, 3)                             10         0.9                          274                 55.1               56.705               65.4009                          4000            18.0602                          1000\n",
      "            1  (1, 2)                             10         0.35                        2385                 81.4               80.4082            1993.02                            4000           462.171                           1000\n",
      "            1  (1, 2)                             10         0.5                         3407                 80.4               78.8               2958.5                             4000           671.238                           1000\n",
      "            1  (1, 2)                             10         0.75                        5111                 81.2               78.9062            5226.3                             4000          1055.64                            1000\n",
      "            1  (1, 2)                             35         0.35                         714                 80.2               78.9474             292.731                           4000            77.8478                          1000\n",
      "            1  (1, 2)                             35         0.5                         1021                 79.9               78.5859             663.975                           4000           157.118                           1000\n",
      "            1  (1, 2)                             35         0.75                        1531                 80.5               78.8423            1214.86                            4000           290.132                           1000\n",
      "            1  (1, 2)                             60         0.35                         428                 78.9               77.5758             108.678                           4000            28.4955                          1000\n",
      "            1  (1, 2)                             60         0.5                          611                 80.3               78.8732             213.966                           4000            56.5715                          1000\n",
      "            1  (1, 2)                             60         0.75                         917                 80.3               78.4158             551.918                           4000           131.265                           1000\n",
      "            1  (1, 3)                             10         0.35                        2492                 80.2               78.9474            2071.07                            4000           486.618                           1000\n",
      "            1  (1, 3)                             10         0.5                         3560                 80.1               78.2178            3135.76                            4000           697.606                           1000\n",
      "            1  (1, 3)                             10         0.75                        5340                 81.3               80                 5889.55                            4000          1122.06                            1000\n",
      "            1  (1, 3)                             35         0.35                         726                 79.7               78.3838             314.999                           4000            81.9026                          1000\n",
      "            1  (1, 3)                             35         0.5                         1038                 81                 80.123              706.722                           4000           169.179                           1000\n",
      "            1  (1, 3)                             35         0.75                        1557                 79.6               77.668             1254.38                            4000           296.969                           1000\n",
      "            1  (1, 3)                             60         0.35                         432                 79.2               77.9352             113.363                           4000            30.3448                          1000\n",
      "            1  (1, 3)                             60         0.5                          618                 79.9               78.8187             219.558                           4000            57.9251                          1000\n",
      "            1  (1, 3)                             60         0.75                         927                 80.6               79                  566.62                            4000           135.978                           1000\n",
      "            2  (1, 1)                             10         0.35                        1598                 82.1               79.6117             252.068                           4000            63.4314                          1000\n",
      "            2  (1, 1)                             10         0.5                         2284                 80.6               77.4621             380.046                           4000            90.2808                          1000\n",
      "            2  (1, 1)                             10         0.75                        3426                 79.6               75.9259             593.369                           4000           146.885                           1000\n",
      "            2  (1, 1)                             35         0.35                         631                 81.2               80.8333             103.995                           4000            26.8735                          1000\n",
      "            2  (1, 1)                             35         0.5                          901                 81.8               78.9272             149.746                           4000            38.3191                          1000\n",
      "            2  (1, 1)                             35         0.75                        1352                 80.8               77.4436             223.797                           4000            56.5943                          1000\n",
      "            2  (1, 1)                             60         0.35                         400                 81.4               82.5328              66.2682                          4000            17.2985                          1000\n",
      "            2  (1, 1)                             60         0.5                          572                 80.2               77.0833              95.7786                          4000            24.6957                          1000\n",
      "            2  (1, 1)                             60         0.75                         858                 79.9               76.8501             142.295                           4000            36.3843                          1000\n",
      "            2  (2, 2)                             10         0.35                         786                 75                 79.1045             127.562                           4000            32.6718                          1000\n",
      "            2  (2, 2)                             10         0.5                         1123                 75.9               77.6765             187.616                           4000            47.5442                          1000\n",
      "            2  (2, 2)                             10         0.75                        1685                 75                 76.1161             292.026                           4000            72.3149                          1000\n",
      "            2  (2, 2)                             35         0.75                         179                 67.5               68.4455              29.5043                          4000             8.1824                          1000\n",
      "            2  (2, 2)                             35         0.85                         203                 68.3               69.6471              33.0708                          4000             9.18338                         1000\n",
      "            2  (2, 2)                             35         0.9                          215                 68.4               69.0045              35.8489                          4000             9.72083                         1000\n",
      "            2  (3, 3)                             10         0.75                         228                 54.8               56.6667              38.3132                          4000            12.183                           1000\n",
      "            2  (3, 3)                             10         0.85                         259                 55.9               58.8477              51.1671                          4000            11.8405                          1000\n",
      "            2  (3, 3)                             10         0.9                          274                 55.2               57.2                 46.9164                          4000            12.4821                          1000\n",
      "            2  (1, 2)                             10         0.35                        2385                 82.8               80.4688             408.092                           4000           102.648                           1000\n",
      "            2  (1, 2)                             10         0.5                         3407                 80.1               76.6355             579.758                           4000           145.118                           1000\n",
      "            2  (1, 2)                             10         0.75                        5111                 81                 77.3234             868.242                           4000           220.207                           1000\n",
      "            2  (1, 2)                             35         0.35                         714                 82.1               81.9706             116.501                           4000            30.0665                          1000\n",
      "            2  (1, 2)                             35         0.5                         1021                 80.4               77.4809             166.144                           4000            45.4299                          1000\n",
      "            2  (1, 2)                             35         0.75                        1531                 79.8               77.0115             250.056                           4000            63.3634                          1000\n",
      "            2  (1, 2)                             60         0.35                         428                 80.9               83.8337              70.5757                          4000            18.9135                          1000\n",
      "            2  (1, 2)                             60         0.5                          611                 81.5               78.9168             100.352                           4000            28.8081                          1000\n",
      "            2  (1, 2)                             60         0.75                         917                 79.3               76.7892             152.902                           4000            38.1916                          1000\n",
      "            2  (1, 3)                             10         0.35                        2492                 82.1               79.7271             429.384                           4000           106.691                           1000\n",
      "            2  (1, 3)                             10         0.5                         3560                 81.4               78.2197             610.876                           4000           152.647                           1000\n",
      "            2  (1, 3)                             10         0.75                        5340                 81.4               77.7985             906.277                           4000           227.693                           1000\n",
      "            2  (1, 3)                             35         0.35                         726                 82.4               82.0833             117.876                           4000            29.8598                          1000\n",
      "            2  (1, 3)                             35         0.5                         1038                 80.5               77.6291             165.923                           4000            42.434                           1000\n",
      "            2  (1, 3)                             35         0.75                        1557                 80.4               77.1698             251.075                           4000            62.5529                          1000\n",
      "            2  (1, 3)                             60         0.35                         432                 81.5               83.9002              69.5619                          4000            18.3413                          1000\n",
      "            2  (1, 3)                             60         0.5                          618                 80.2               78.0392             100.414                           4000            25.5393                          1000\n",
      "            2  (1, 3)                             60         0.75                         927                 79.7               77.176              154.32                            4000            38.3515                          1000\n",
      "\n",
      "Vector Type Key: 0 for COUNT VECTOR, 1 for BINARY COUNT VECTOR, 2 for NORMALIZED COUNT VECTOR(tf-idf)\n",
      "NGRAM Key: 1 for UNIGRAMS, 2 for BIGRAMS, 3 for TRIGRAMS, (1,2) for (UNIGRAMS & BIGRAMS), (1,3) for (UNIGRAMS,BIGRAMS, & TRIGRAMS)\n",
      "\n",
      "Best Hyperparameters: VECTOR TYPE:  2 , NGRAM:  (1, 2) , MINIMUM DOCUMENT FREQUENCY:  10 , K PERCENT:  0.35 , FINAL NUMBER OF FEATURES:  2385\n",
      "Best One-Shot Accuracy:  82.8\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "vector_types = [0, 1, 2] # 0 for COUNT VECTOR, 1 for BINARY COUNT VECTOR, 2 for NORMALIZED COUNT VECTOR(tf-idf)\n",
    "ngram_types = [ (1,1), (2,2), (3,3), (1,2), (1,3)] # (1,1) for UNIGRAMS, (2,2) for BIGRAMS, (3,3) for TRIGRAMS, (1,2) for (UNIGRAMS,BIGRAMS), (1,3) for (UNIGRAMS,TRIGRAMS) \n",
    "if dataset == 0:\n",
    "    mindf_values=[3, 5, 10]\n",
    "else:\n",
    "    mindf_values=[10, 35, 60]\n",
    "\n",
    "k_greater_10k = [0.05, 0.07, 0.10]\n",
    "k_greater_1k = [0.35, 0.50, 0.75]\n",
    "k_lessThan_1k = [0.75, 0.85, 0.90]\n",
    "\n",
    "# Optimal Result Initializations\n",
    "best_acc=0\n",
    "vector_type_best=0\n",
    "mindf_best=0\n",
    "ngram_type_best=0\n",
    "k_best=0\n",
    "numfeat_best=0\n",
    "\n",
    "best_assocMem=[]\n",
    "itemMem_best=[]\n",
    "Train_best=[]\n",
    "Test_best=[]\n",
    "\n",
    "# Generate Table Initialization\n",
    "table_data=[]\n",
    "col_names = [\"VECTOR TYPE\", \"NGRAM\", \"MINIMUM DOCUMENT FREQUENCY\", \"K PERCENT\", \"FINAL NUMBER OF FEATURES\", \"ONE-SHOT ACCURACY\", \"ONE-SHOT-PRECISION\", \"TRAINING TIME (s)\", \"NUMBER OF TRAINING SAMPLES\", \"TESTING TIME (s)\", \"NUMBER OF TESTING SAMPLES\"]\n",
    "\n",
    "for vector_type in tqdm_notebook(vector_types):\n",
    "    for ngram in tqdm_notebook(ngram_types):\n",
    "        for mindf in mindf_values:\n",
    "            \n",
    "            # Build Vectorizer With Desired Hyperparameters\n",
    "            if (vector_type==0):\n",
    "                cnt = CountVectorizer(analyzer=\"word\", ngram_range=ngram, min_df=mindf)\n",
    "            elif(vector_type==1):\n",
    "                cnt = CountVectorizer(analyzer=\"word\", ngram_range=ngram, min_df=mindf, binary=True)\n",
    "            elif (vector_type==2):\n",
    "                cnt = TfidfVectorizer(analyzer=\"word\", ngram_range=ngram, min_df=mindf)\n",
    "\n",
    "            try:\n",
    "                # Fit Vectorizer and Transform Training/Testing Sets\n",
    "                TrainXdf_temp = cnt.fit_transform(TrainXdf).toarray()\n",
    "                TestXdf_temp = cnt.transform(TestXdf).toarray()\n",
    "\n",
    "            except (ValueError):  \n",
    "                if (vector_type==0):\n",
    "                    cnt = CountVectorizer(analyzer=\"word\", ngram_range=ngram)\n",
    "                    mindf=\"-\"\n",
    "                elif(vector_type==1):\n",
    "                    cnt = CountVectorizer(analyzer=\"word\", ngram_range=ngram, binary=True)\n",
    "                    mindf=\"-\"\n",
    "                elif (vector_type==2):\n",
    "                    cnt = TfidfVectorizer(analyzer=\"word\", ngram_range=ngram)\n",
    "                    mindf=\"-\"\n",
    "                \n",
    "                # Fit Vectorizer and Transform Training/Testing Sets\n",
    "                TrainXdf_temp = cnt.fit_transform(TrainXdf).toarray()\n",
    "                TestXdf_temp = cnt.transform(TestXdf).toarray()\n",
    "\n",
    "            # Get % of Current Features want SelectKBest to Keep:\n",
    "            if ( TrainXdf_temp.shape[1] >= 10000 ):\n",
    "                k_values=copy.copy(k_greater_10k)\n",
    "            elif ( TrainXdf_temp.shape[1] >= 1000 ):\n",
    "                k_values=copy.copy(k_greater_1k)\n",
    "            elif ( TrainXdf_temp.shape[1] >= 100 ):\n",
    "                k_values=copy.copy(k_lessThan_1k)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            for k_val in k_values:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"error\")\n",
    "                    try:\n",
    "                        # Feature Selection using Chi2\n",
    "                        Selector=SelectKBest(chi2, k=int(k_val*TrainXdf_temp.shape[1]))\n",
    "                        TrainXdf_temp1= Selector.fit_transform(TrainXdf_temp, TrainYdf)\n",
    "                        TestXdf_temp1 = Selector.transform(TestXdf_temp)\n",
    "                    except UserWarning:\n",
    "                        TrainXdf_temp1=copy.copy(TrainXdf_temp)\n",
    "                        TestXdf_temp1=copy.copy(TestXdf_temp)\n",
    "                        k_val=\"-\"\n",
    "\n",
    "                # Generate Item Memory\n",
    "                itemMem=itemMemGen(HV_dim, TrainXdf_temp1.shape[1])\n",
    "\n",
    "                # Train Model (i.e. Generate Model's Associative Memory)\n",
    "                t0=time.time()\n",
    "                assocMem = train(TrainXdf_temp1, TrainYdf, itemMem, HV_dim, len(sent_count))\n",
    "                t1=time.time()\n",
    "                train_time = t1-t0\n",
    "\n",
    "                # One-Shot Training Results\n",
    "                t0=time.time()\n",
    "                one_shot_accuracy, one_shot_precision = test(itemMem, assocMem, TestXdf_temp1, TestYdf)\n",
    "                t1=time.time()\n",
    "                test_time = t1-t0\n",
    "\n",
    "                # Add Data to Table\n",
    "                data = [vector_type, ngram, mindf, k_val, TrainXdf_temp1.shape[1], one_shot_accuracy, one_shot_precision, train_time, len(TrainYdf), test_time, len(TestYdf)]\n",
    "                table_data.append(data)\n",
    "\n",
    "                if one_shot_accuracy>best_acc:\n",
    "                    best_acc=one_shot_accuracy\n",
    "                    best_assocMem=copy.copy(assocMem)\n",
    "                    itemMem_best=copy.copy(itemMem)\n",
    "                    Train_best=copy.copy(TrainXdf_temp1)\n",
    "                    Test_best=copy.copy(TestXdf_temp1)\n",
    "                    \n",
    "                    cnt_best=cnt\n",
    "                    vector_type_best=vector_type\n",
    "                    mindf_best=mindf\n",
    "                    ngram_type_best=ngram\n",
    "                    k_best=k_val\n",
    "                    numfeat_best=TrainXdf_temp1.shape[1]\n",
    "\n",
    "# Get Necessary Components for Best Model\n",
    "assocMem=copy.copy(best_assocMem)\n",
    "itemMem=copy.copy(itemMem_best)\n",
    "TrainXdf=copy.copy(Train_best)\n",
    "TestXdf=copy.copy(Test_best)\n",
    "\n",
    "# Save Results to File\n",
    "df=pd.DataFrame(table_data, columns=col_names)\n",
    "filepath=\"./Results/HyperparameterResults/VECTORIZED_\" + str(dataset) +\".csv\"\n",
    "df.to_csv(filepath)\n",
    "            \n",
    "print(\"Results of Various Hyperparameter Sets:\")\n",
    "print(tabulate(table_data, headers=col_names, tablefmt=\"simple\"))\n",
    "print(\"\\nVector Type Key: 0 for COUNT VECTOR, 1 for BINARY COUNT VECTOR, 2 for NORMALIZED COUNT VECTOR(tf-idf)\")\n",
    "print(\"NGRAM Key: 1 for UNIGRAMS, 2 for BIGRAMS, 3 for TRIGRAMS, (1,2) for (UNIGRAMS & BIGRAMS), (1,3) for (UNIGRAMS,BIGRAMS, & TRIGRAMS)\\n\")\n",
    "print(\"Best Hyperparameters: VECTOR TYPE: \", vector_type_best, \", NGRAM: \", ngram_type_best, \", MINIMUM DOCUMENT FREQUENCY: \", mindf_best, \", K PERCENT: \", k_best, \", FINAL NUMBER OF FEATURES: \", numfeat_best)\n",
    "print(\"Best One-Shot Accuracy: \", best_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retraining Optimal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Retraining Model w Learning Parameter:  20  Epochs --------\n",
      "Epoch  1 :  82.6\n",
      "Epoch  2 :  83.89999999999999\n",
      "Epoch  3 :  83.1\n",
      "Epoch  4 :  82.89999999999999\n",
      "Epoch  5 :  83.3\n",
      "Epoch  6 :  85.7\n",
      "Epoch  7 :  83.7\n",
      "Epoch  8 :  84.5\n",
      "Epoch  9 :  83.7\n",
      "Epoch  10 :  85.9\n",
      "Epoch  11 :  86.1\n",
      "Epoch  12 :  85.3\n",
      "Epoch  13 :  84.1\n",
      "Epoch  14 :  85.7\n",
      "Epoch  15 :  85.5\n",
      "Epoch  16 :  85.9\n",
      "Epoch  17 :  85.9\n",
      "Epoch  18 :  86.2\n",
      "Epoch  19 :  86.1\n",
      "Epoch  20 :  86.1\n",
      "-------- Retraining Model without Learning Parameter:  20  Epochs --------\n",
      "Epoch  1 :  86.1\n",
      "Epoch  2 :  87.2\n",
      "Epoch  3 :  87.3\n",
      "Epoch  4 :  86.8\n",
      "Epoch  5 :  86.8\n",
      "Epoch  6 :  87.2\n",
      "Epoch  7 :  87.0\n",
      "Epoch  8 :  86.5\n",
      "Epoch  9 :  86.4\n",
      "Epoch  10 :  86.4\n",
      "Epoch  11 :  86.4\n",
      "Epoch  12 :  86.2\n",
      "Epoch  13 :  86.2\n",
      "Epoch  14 :  86.1\n",
      "Epoch  15 :  86.6\n",
      "Epoch  16 :  86.3\n",
      "Epoch  17 :  86.4\n",
      "Epoch  18 :  86.1\n",
      "Epoch  19 :  86.2\n",
      "Epoch  20 :  86.7\n"
     ]
    }
   ],
   "source": [
    "### Function: VECTORIZED HDC Re-Training Function that creates a New Associative Memory for the Model\n",
    "    ## Inputs: X:           Training Samples\n",
    "    ##         Y:           Outputs of Training Samples\n",
    "    ##         itemMem:     Generated Item Memory\n",
    "    ##         assocMem:    Associative Memory of Current Model\n",
    "    ##         alpha:       Learning Rate Parameter\n",
    "    ## Output: assocMem: New Associative Memory\n",
    "def retrain(X, Y, itemMem, assocMem, alpha):\n",
    "    sample_index = 0\n",
    "    for sample in X:\n",
    "        sample_HV = encode(sample, itemMem)\n",
    "        prediction = get_prediction(assocMem, sample_HV)\n",
    "        if prediction != Y[sample_index]:\n",
    "            assocMem[Y[sample_index]] = np.add(assocMem[Y[sample_index]], alpha * sample_HV)\n",
    "            assocMem[prediction] = np.subtract(assocMem[prediction], alpha * sample_HV)\n",
    "        sample_index += 1\n",
    "    return assocMem\n",
    "\n",
    "# Re-Train Optimal Model with Learning Parameter\n",
    "learningparam_results=[]\n",
    "num_epochs = 20\n",
    "print('-------- Retraining Model w Learning Parameter: ', num_epochs, ' Epochs --------')\n",
    "for epoch in range(num_epochs):\n",
    "    assocMem = retrain(TrainXdf, TrainYdf, itemMem, assocMem, alpha = num_epochs - epoch)\n",
    "    acc, prec = test(itemMem, assocMem, TestXdf, TestYdf)\n",
    "    print('Epoch ', (epoch+1), ': ', acc)\n",
    "    learningparam_results.append([acc,prec])\n",
    "\n",
    "# Re-Train Optimal Model without Learning Parameter\n",
    "assocMem=copy.copy(best_assocMem)\n",
    "no_learningparam_results=[]\n",
    "print('-------- Retraining Model without Learning Parameter: ', num_epochs, ' Epochs --------')\n",
    "for epoch in range(num_epochs):\n",
    "    assocMem = retrain(TrainXdf, TrainYdf, itemMem, assocMem, alpha = 1)\n",
    "    acc, prec = test(itemMem, assocMem, TestXdf, TestYdf)\n",
    "    print('Epoch ', (epoch+1), ': ', acc)\n",
    "    no_learningparam_results.append([acc,prec])\n",
    "\n",
    "# Save All Results to Files\n",
    "col_name=[\"Accuracy\", \"Precision\"]\n",
    "\n",
    "df_lp=pd.DataFrame(learningparam_results, columns=col_name)\n",
    "filepath=\"./Results/EpochResults_LearningParam/VECTORIZED_\" + str(dataset) + \".csv\"\n",
    "df_lp.to_csv(filepath)\n",
    "\n",
    "df_nlp=pd.DataFrame(no_learningparam_results, columns=col_name)\n",
    "filepath=\"./Results/EpochResults_NoLearningParam/VECTORIZED_\" + str(dataset) + \".csv\"\n",
    "df_nlp.to_csv(filepath)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "071eaeccc96c6410cecdb330bf8e8ae0267d24b86e05481c728d399cbe7cbc33"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('aml': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
