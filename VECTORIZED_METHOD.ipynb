{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VECTORIZED HDC METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Basic Packages\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import time\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import sent_tokenize,word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from tabulate import tabulate\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Insights and Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Sentiment140 Twitter Dataset...\n",
      "Total number of Samples In Dataset: 1599999\n"
     ]
    }
   ],
   "source": [
    "### Function: Collects sample data from files in Stanford Dataset Subfolders\n",
    "    ## Inputs: folderpath: Path to Desired Folder\n",
    "    ##         sentiment:  Sentiment Value (0 or 1)\n",
    "    ## Output: df: Pandas Dataframe of all Sample Data found in desired folder\n",
    "def stanfordDatasetFolderDataLoader(folderpath, sentiment):\n",
    "    file_list=listdir(folderpath)\n",
    "\n",
    "    df = pd.DataFrame(columns = ['Review', 'Sentiment'])\n",
    "    for file in file_list:\n",
    "        filepath=folderpath + file\n",
    "        f = open(filepath,'r', encoding=\"utf-8\")\n",
    "        sample = f.read()\n",
    "        f.close()\n",
    "        df = df.append({'Review' : sample, 'Sentiment' : sentiment}, ignore_index = True)\n",
    "    return df\n",
    "\n",
    "# Choose Dataset (0: Sentiment140, 1: Stanford IMBD Dataset)\n",
    "dataset = 0\n",
    "\n",
    "# Load Dataset\n",
    "if (dataset==0):\n",
    "    print('Using Sentiment140 Twitter Dataset...')\n",
    "    # Read in Sentiment140 data from CSV\n",
    "    df = pd.read_csv('./Sentiment140_Tweets/data.csv')\n",
    "    df.columns =['Sentiment', 'IDs', 'Date', 'Flag', 'User', 'Tweet']\n",
    "else:\n",
    "    print('Using Stanford IMBD Movie Review Dataset...')\n",
    "    # Read in Training Stanford IMBD Movie Review data from subfolders\n",
    "    train_pos=stanfordDatasetFolderDataLoader('./StanfordMovie/train/pos/',1)\n",
    "    train_neg=stanfordDatasetFolderDataLoader('./StanfordMovie/train/neg/',0)\n",
    "    train_df=pd.concat([train_pos, train_neg], axis=0)\n",
    "    \n",
    "    # Read in Testing Stanford IMBD Movie Review data from subfolders\n",
    "    test_pos=stanfordDatasetFolderDataLoader('./StanfordMovie/test/pos/',1)\n",
    "    test_neg=stanfordDatasetFolderDataLoader('./StanfordMovie/test/neg/',0)\n",
    "    test_df=pd.concat([test_pos, test_neg], axis=0)\n",
    "\n",
    "    df=pd.concat([train_df, test_df], axis=0)\n",
    "print('Total number of Samples In Dataset:', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset in Use has No NULL values.\n",
      "Dataset Length after Cleanup: 1599999\n"
     ]
    }
   ],
   "source": [
    "# Dataset Cleanup:\n",
    "\n",
    "# Sentiment140 Sentiment Clean Up\n",
    "if dataset==0:\n",
    "    # Replace Sentiment of 4 (Positive) with 1\n",
    "    df[\"Sentiment\"].replace({4: 1}, inplace=True)\n",
    "    # Eliminate Neutral Tweets, if any\n",
    "    df = df[df['Sentiment'] != 2]\n",
    "\n",
    "# Check for Null Values\n",
    "if ( not df.isnull().values.any() ):\n",
    "    print(\"Dataset in Use has No NULL values.\")\n",
    "else:\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "print(\"Dataset Length after Cleanup:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>IDs</th>\n",
       "      <th>Date</th>\n",
       "      <th>Flag</th>\n",
       "      <th>User</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1016244</th>\n",
       "      <td>1</td>\n",
       "      <td>1881672289</td>\n",
       "      <td>Fri May 22 05:16:44 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>viry_trivium</td>\n",
       "      <td>Happy birthday, sister!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1303317</th>\n",
       "      <td>1</td>\n",
       "      <td>2009051656</td>\n",
       "      <td>Tue Jun 02 15:04:22 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Earlthedog</td>\n",
       "      <td>Just finished eating supper and now I am attac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576684</th>\n",
       "      <td>0</td>\n",
       "      <td>2211886069</td>\n",
       "      <td>Wed Jun 17 13:24:27 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>StefyyMarie</td>\n",
       "      <td>i hate love right now.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837327</th>\n",
       "      <td>1</td>\n",
       "      <td>1558734942</td>\n",
       "      <td>Sun Apr 19 09:15:07 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tezzer57</td>\n",
       "      <td>Photo fest in LDN, Tudor feast last night, don...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985344</th>\n",
       "      <td>1</td>\n",
       "      <td>1834470136</td>\n",
       "      <td>Mon May 18 03:03:30 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>dave_sherratt</td>\n",
       "      <td>@piercedbrat happy bday for tomoz, all the bes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1369789</th>\n",
       "      <td>1</td>\n",
       "      <td>2050886442</td>\n",
       "      <td>Fri Jun 05 19:28:18 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>thaisprudencio</td>\n",
       "      <td>today was awesome!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587089</th>\n",
       "      <td>0</td>\n",
       "      <td>2216194514</td>\n",
       "      <td>Wed Jun 17 19:09:39 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>alwyshoutashley</td>\n",
       "      <td>I wish it would stop raining. I'm ready for th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46597</th>\n",
       "      <td>0</td>\n",
       "      <td>1677444411</td>\n",
       "      <td>Sat May 02 02:06:29 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>kasey79</td>\n",
       "      <td>@DannyGirlAlways Ok I still feel kind of bad t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1409132</th>\n",
       "      <td>1</td>\n",
       "      <td>2055829198</td>\n",
       "      <td>Sat Jun 06 10:01:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Bockman13</td>\n",
       "      <td>Hanging with Anna and Fernando!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984326</th>\n",
       "      <td>1</td>\n",
       "      <td>1834375043</td>\n",
       "      <td>Mon May 18 02:41:35 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AberdeenUK</td>\n",
       "      <td>Good morning everyone. FYI bought him Rock Ban...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sentiment         IDs                          Date      Flag  \\\n",
       "1016244          1  1881672289  Fri May 22 05:16:44 PDT 2009  NO_QUERY   \n",
       "1303317          1  2009051656  Tue Jun 02 15:04:22 PDT 2009  NO_QUERY   \n",
       "576684           0  2211886069  Wed Jun 17 13:24:27 PDT 2009  NO_QUERY   \n",
       "837327           1  1558734942  Sun Apr 19 09:15:07 PDT 2009  NO_QUERY   \n",
       "985344           1  1834470136  Mon May 18 03:03:30 PDT 2009  NO_QUERY   \n",
       "...            ...         ...                           ...       ...   \n",
       "1369789          1  2050886442  Fri Jun 05 19:28:18 PDT 2009  NO_QUERY   \n",
       "587089           0  2216194514  Wed Jun 17 19:09:39 PDT 2009  NO_QUERY   \n",
       "46597            0  1677444411  Sat May 02 02:06:29 PDT 2009  NO_QUERY   \n",
       "1409132          1  2055829198  Sat Jun 06 10:01:49 PDT 2009  NO_QUERY   \n",
       "984326           1  1834375043  Mon May 18 02:41:35 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    User                                              Tweet  \n",
       "1016244     viry_trivium                           Happy birthday, sister!   \n",
       "1303317       Earlthedog  Just finished eating supper and now I am attac...  \n",
       "576684       StefyyMarie                            i hate love right now.   \n",
       "837327          tezzer57  Photo fest in LDN, Tudor feast last night, don...  \n",
       "985344     dave_sherratt  @piercedbrat happy bday for tomoz, all the bes...  \n",
       "...                  ...                                                ...  \n",
       "1369789   thaisprudencio                                today was awesome!   \n",
       "587089   alwyshoutashley  I wish it would stop raining. I'm ready for th...  \n",
       "46597            kasey79  @DannyGirlAlways Ok I still feel kind of bad t...  \n",
       "1409132        Bockman13                   Hanging with Anna and Fernando!   \n",
       "984326        AberdeenUK  Good morning everyone. FYI bought him Rock Ban...  \n",
       "\n",
       "[5000 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downsample the Dataset to 5,000 Total Samples\n",
    "if (dataset==0):\n",
    "    percentage = 0.003125\n",
    "else:\n",
    "    percentage = 0.1\n",
    "df_downsampled = df.sample(frac=percentage,random_state=0)\n",
    "df_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where Sentiment is Positive when Sentiment Value = 1 and Negative when Sentiment Value = 0\n"
     ]
    }
   ],
   "source": [
    "# Visualization of Dataset Sentiment Outcomes - Ensured Even Distribution of Outcomes\n",
    "sent_count = df_downsampled['Sentiment'].value_counts()\n",
    "plt.figure(figsize=(8, 5))\n",
    "w = 0.35  \n",
    "plt.bar(x=np.arange(len(sent_count)), height=sent_count, width = w)\n",
    "plt.xticks(np.arange(len(sent_count)), sent_count.index.tolist())\n",
    "\n",
    "if dataset==0:\n",
    "    plt.xlabel('Tweet Sentiment Value')\n",
    "    plt.ylabel('Tweet Sentiment Value Count')\n",
    "    plt.title('Sentiment140 Twitter Dataset Sentiment Value Frequency')\n",
    "else:\n",
    "    plt.xlabel('Movie Review Sentiment Value')\n",
    "    plt.ylabel('Movie Review Sentiment Value Count')\n",
    "    plt.title('Stanford IMBD Movie Review Dataset Sentiment Value Frequency')\n",
    "plt.show()\n",
    "print('Where Sentiment is Positive when Sentiment Value = 1 and Negative when Sentiment Value = 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sorry always going bed bye',\n",
       " 'gon na fix nail polish auditorium isnt best place paint nail riding berts brei',\n",
       " 'watch end season see think need rewatch season get',\n",
       " 'fail stop writing ray bradbury lt love quote']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Function: Pre-Processes Samples\n",
    "    ## Inputs: samples:        Array of samples\n",
    "    ##         sample_results: Sentiment of Input Samples\n",
    "    ## Outputs: pre_procc_samps: Array of samples pre-processed\n",
    "    ##          pre_procc_res:   Array of results for pre-processed samples      \n",
    "def PreProcess(samples, sample_results):\n",
    "    pre_procc_samps = []\n",
    "    pre_procc_res=[]\n",
    "\n",
    "    # Storing all punctuations using RE library like !;,\"% etc\n",
    "    re_puncs = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # Storing all stop words like a, an, the, when, there, this etc\n",
    "    stop_word  = set(stopwords.words('english'))\n",
    "    # Lemmatizing object\n",
    "    lem = WordNetLemmatizer()\n",
    "\n",
    "    index=0\n",
    "    for sample in samples:\n",
    "        \n",
    "        # Get rid or URLs and Twitter Handles\n",
    "        sample = re.sub(r\"http\\S+\", \"\", sample)\n",
    "        sample = re.sub(r\"@\\S+\", \"\", str(sample))\n",
    "\n",
    "        # Get words in sample\n",
    "        words = word_tokenize(str(sample))\n",
    "\n",
    "        # Converting all characters to lower case\n",
    "        words_lower = [w.lower() for w in words]              \n",
    "\n",
    "        # Remove all punctuation\n",
    "        words_lower_no_punc = [re_puncs.sub('', w) for w in words_lower]\n",
    "\n",
    "        # Keep only alpha words\n",
    "        words_lower_alpha = [i for i in words_lower_no_punc if i.isalpha()]\n",
    "\n",
    "        # Removing all stop words\n",
    "        words_lower_alpha_nostop = [w for w in words_lower_alpha if w not in stop_word]\n",
    "\n",
    "        # Doing Lemmatizing of words\n",
    "        words_lower_alpha_nostop_lemma = [lem.lemmatize(w) for w in words_lower_alpha_nostop]\n",
    "\n",
    "        # Convert back to string and (possibly) one-hot encode tweet\n",
    "        pre_procc_str = ' '.join(words_lower_alpha_nostop_lemma)\n",
    "        if (pre_procc_str != \"\"):\n",
    "            pre_procc_samps.append(pre_procc_str)\n",
    "            pre_procc_res.append(sample_results[index])\n",
    "        index=index+1\n",
    "        \n",
    "    return pre_procc_samps, pre_procc_res\n",
    "\n",
    "# Pre-Proccess the Dataset\n",
    "if dataset==0:\n",
    "    Xdf, Ydf = PreProcess(df_downsampled['Tweet'].to_numpy(), df_downsampled['Sentiment'].to_numpy())\n",
    "else:\n",
    "    Xdf, Ydf = PreProcess(df_downsampled['Review'].to_numpy(), df_downsampled['Sentiment'].to_numpy())\n",
    "\n",
    "# Get Final Train/Test Sets:\n",
    "TrainXdf,TestXdf, TrainYdf, TestYdf = train_test_split(Xdf, Ydf, test_size=.3, random_state=2)\n",
    "TrainYdf=np.array(TrainYdf)\n",
    "TestYdf=np.array(TestYdf)\n",
    "TrainXdf[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Training Initialization\n",
    "### Item Memory Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function: Item Memory Generation\n",
    "    ## Inputs: dim: Number of columns (i.e. length of HV)\n",
    "    ##         num_char: Number of rows (i.e. number of features)\n",
    "    ## Output: dictMem: Item Memory containing HVs for each supported char\n",
    "def itemMemGen(dim=10000, num_char=37):\n",
    "    dictMem = np.random.randint(2, size=(num_char, dim), dtype='int32')\n",
    "    dictMem[dictMem == 0] = -1\n",
    "    return dictMem\n",
    "\n",
    "# Parameters:\n",
    "HV_dim = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function: Encodes a Sample into a HV using VECTORIZED HDC Approach\n",
    "    ## Inputs: sample:      Training Sample\n",
    "    ##         itemMem:     Generate Item Memory\n",
    "    ## Output: sample_HV: HV of inputted sample \n",
    "def encode(sample, itemMem):\n",
    "    return sample.dot(itemMem)\n",
    "\n",
    "### Function: VECTORIZED HDC Training Function that creates an Associative Memory for the Model\n",
    "    ## Inputs: X:           Training Samples\n",
    "    ##         Y:           Outputs of Training Samples\n",
    "    ##         itemMem:     Generated Item Memory\n",
    "    ##         HV_dim:      Dimension of HV\n",
    "    ##         sent_count:  Number of Possible Sentiment Values\n",
    "    ## Output: assocMem: Associative Memory \n",
    "def train(X, Y, itemMem, HV_dim, sent_count):\n",
    "    assocMem = np.zeros((sent_count, HV_dim), dtype='int32')\n",
    "    sample_idx = 0\n",
    "    \n",
    "    for sample in X:\n",
    "        review_HV = encode(sample, itemMem)\n",
    "        assocMem[Y[sample_idx]] = np.add(assocMem[Y[sample_idx]], review_HV)\n",
    "        sample_idx += 1\n",
    "\n",
    "    return assocMem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function: Compares Input HV to Class HVs and Returns the Predicted Class\n",
    "    ## Inputs: assocMem: Model's Associative Memory\n",
    "    ##         inputHV:  Encoded HV of a sample\n",
    "    ## Output: pred: the predicted class\n",
    "def get_prediction(assocMem, inputHV):\n",
    "    pred = assocMem[0]\n",
    "    maximum = np.NINF\n",
    "\n",
    "    for index in range(len(assocMem)):\n",
    "        similarity = cosine_similarity([inputHV, assocMem[index]])[0][1]  \n",
    "        if (similarity > maximum):\n",
    "            pred = index\n",
    "            maximum = similarity\n",
    "\n",
    "    return pred\n",
    "\n",
    "### Function: Tests the VECTORIZED HDC Model and Returns Accuracy of Model\n",
    "    ## Inputs: itemMem:     Generated Item Memory\n",
    "    ##         assocMem:    Model's Associative Memory\n",
    "    ##         TestXdf:     Test Samples\n",
    "    ##         TextYdf:     Sentiment of Test Samples\n",
    "    ## Output: accuracy: Accuracy of the Model\n",
    "def test(itemMem, assocMem, TestXdf, TestYdf):\n",
    "    correct_count = 0\n",
    "\n",
    "    for index in range(len(TestXdf)):\n",
    "        prediction = get_prediction(assocMem, encode(TestXdf[index], itemMem))\n",
    "        if (TestYdf[index] == prediction):\n",
    "            correct_count += 1\n",
    "            \n",
    "    accuracy = (correct_count / len(TestYdf)) * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Hyperparameter Search\n",
    "### One-Shot Training/Accuracy of Various Sets of Hyperparameter Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Various Hyperparameter Sets:\n",
      "  VECTOR TYPE    MAX FEATURES  NGRAM      ONE-SHOT ACCURACY    TRAINING TIME (s)    NUMBER OF TRAINING SAMPLES    TESTING TIME (s)    NUMBER OF TESTING SAMPLES\n",
      "-------------  --------------  -------  -------------------  -------------------  ----------------------------  ------------------  ---------------------------\n",
      "            0             300  (1, 1)               64.7256              57.1328                          3484             28.2679                         1494\n",
      "            0             300  (2, 2)               51.004               58.8768                          3484             28.0999                         1494\n",
      "            0             300  (3, 3)               49.9331              56.7932                          3484             28.2333                         1494\n",
      "            0             300  (1, 2)               64.3909              56.6563                          3484             28.6295                         1494\n",
      "            0             300  (1, 3)               63.9893              56.7887                          3484             28.8292                         1494\n",
      "            0             500  (1, 1)               66.1981              94.6346                          3484             46.3289                         1494\n",
      "            0             500  (2, 2)               51.2048              94.1993                          3484             46.3484                         1494\n",
      "            0             500  (3, 3)               50                   95.2266                          3484             47.0042                         1494\n",
      "            0             500  (1, 2)               65.5957              96.3296                          3484             47.1059                         1494\n",
      "            0             500  (1, 3)               65.5288              95.6605                          3484             47.0856                         1494\n",
      "            0             700  (1, 1)               65.9304             133.048                           3484             65.3055                         1494\n",
      "            0             700  (2, 2)               51.2048             132.689                           3484             65.106                          1494\n",
      "            0             700  (3, 3)               50.1339             134.182                           3484             65.5923                         1494\n",
      "            0             700  (1, 2)               65.7965             133.562                           3484             65.4173                         1494\n",
      "            0             700  (1, 3)               65.9304             133.013                           3484             72.3094                         1494\n",
      "            0             800  (1, 1)               65.7965             155.091                           3484             74.6699                         1494\n",
      "            0             800  (2, 2)               51.004              151.862                           3484             74.3776                         1494\n",
      "            0             800  (3, 3)               49.8661             151.768                           3484             74.0863                         1494\n",
      "            0             800  (1, 2)               65.0602             153.125                           3484             74.6253                         1494\n",
      "            0             800  (1, 3)               65.6627             152.386                           3484             74.3798                         1494\n",
      "            0             900  (1, 1)               65.7965             173.438                           3484             90.9459                         1494\n",
      "            0             900  (2, 2)               51.4056             181.742                           3484             87.1944                         1494\n",
      "            0             900  (3, 3)               50.1339             180.838                           3484             86.8175                         1494\n",
      "            0             900  (1, 2)               66.5328             178.721                           3484             85.9243                         1494\n",
      "            0             900  (1, 3)               65.261              180.84                            3484             86.2229                         1494\n",
      "            1             300  (1, 1)               65.1272              59.6773                          3484             29.0643                         1494\n",
      "            1             300  (2, 2)               50.8701              58.4104                          3484             28.9331                         1494\n",
      "            1             300  (3, 3)               50.2008              58.502                           3484             29.0951                         1494\n",
      "            1             300  (1, 2)               65.1941              59.3089                          3484             28.9344                         1494\n",
      "            1             300  (1, 3)               64.7925              58.6059                          3484             28.8598                         1494\n",
      "            1             500  (1, 1)               66.2651              97.9434                          3484             47.4894                         1494\n",
      "            1             500  (2, 2)               51.071               98.782                           3484             47.5841                         1494\n",
      "            1             500  (3, 3)               49.9331              98.0117                          3484             47.5816                         1494\n",
      "            1             500  (1, 2)               65.7296              98.1591                          3484             47.4353                         1494\n",
      "            1             500  (1, 3)               66.1981             106.067                           3484             47.6313                         1494\n",
      "            1             700  (1, 1)               66.0643             138.176                           3484             66.3052                         1494\n",
      "            1             700  (2, 2)               50.8701             138.422                           3484             66.3307                         1494\n",
      "            1             700  (3, 3)               50.2677             140.289                           3484             66.0962                         1494\n",
      "            1             700  (1, 2)               65.261              137.666                           3484             66.574                          1494\n",
      "            1             700  (1, 3)               65.8635             137.636                           3484             66.2453                         1494\n",
      "            1             800  (1, 1)               64.6586             156.83                            3484             76.0809                         1494\n",
      "            1             800  (2, 2)               51.4726             158.478                           3484             76.7491                         1494\n",
      "            1             800  (3, 3)               50.2008             161.962                           3484             75.6186                         1494\n",
      "            1             800  (1, 2)               65.5288             157.534                           3484             75.5639                         1494\n",
      "            1             800  (1, 3)               64.9933             156.494                           3484             75.9772                         1494\n",
      "            1             900  (1, 1)               66.5328             178.914                           3484             86.9678                         1494\n",
      "            1             900  (2, 2)               51.4726             179.445                           3484             86.8464                         1494\n",
      "            1             900  (3, 3)               50                  177.999                           3484             85.9418                         1494\n",
      "            1             900  (1, 2)               65.6627             179.078                           3484             86.4072                         1494\n",
      "            1             900  (1, 3)               65.261              178.59                            3484             89.1724                         1494\n",
      "            2             300  (1, 1)               57.3628              45.009                           3484             20.1305                         1494\n",
      "            2             300  (2, 2)               51.5395              44.7669                          3484             20.2053                         1494\n",
      "            2             300  (3, 3)               50.1339              44.4058                          3484             20.3709                         1494\n",
      "            2             300  (1, 2)               57.095               44.5951                          3484             20.2336                         1494\n",
      "            2             300  (1, 3)               58.5676              44.5915                          3484             20.2995                         1494\n",
      "            2             500  (1, 1)               56.8273              75.4917                          3484             33.0121                         1494\n",
      "            2             500  (2, 2)               51.6734              74.4795                          3484             33.3384                         1494\n",
      "            2             500  (3, 3)               50.1339              74.1305                          3484             33.111                          1494\n",
      "            2             500  (1, 2)               55.4886              78.5703                          3484             33.1649                         1494\n",
      "            2             500  (1, 3)               56.158               74.4222                          3484             32.4539                         1494\n",
      "            2             700  (1, 1)               54.7523             101.661                           3484             45.5004                         1494\n",
      "            2             700  (2, 2)               51.2048             101.962                           3484             44.838                          1494\n",
      "            2             700  (3, 3)               50.0669             102.565                           3484             45.4311                         1494\n",
      "            2             700  (1, 2)               53.9491             103.332                           3484             46.2124                         1494\n",
      "            2             700  (1, 3)               55.6894             103.638                           3484             46.2301                         1494\n",
      "            2             800  (1, 1)               54.9531             119.777                           3484             52.2956                         1494\n",
      "            2             800  (2, 2)               51.3387             118.875                           3484             52.3497                         1494\n",
      "            2             800  (3, 3)               50.0669             118.111                           3484             51.757                          1494\n",
      "            2             800  (1, 2)               53.079              118.099                           3484             52.7005                         1494\n",
      "            2             800  (1, 3)               53.4806             120.31                            3484             51.8827                         1494\n",
      "            2             900  (1, 1)               53.6145             136.336                           3484             58.6033                         1494\n",
      "            2             900  (2, 2)               50.8032             133.977                           3484             58.3787                         1494\n",
      "            2             900  (3, 3)               50.0669             132.692                           3484             58.2298                         1494\n",
      "            2             900  (1, 2)               53.8822             134.037                           3484             58.3684                         1494\n",
      "            2             900  (1, 3)               53.8822             134.062                           3484             59.1861                         1494\n",
      "\n",
      "Vector Type Key: 0 for COUNT VECTOR, 1 for BINARY COUNT VECTOR, 2 for NORMALIZED COUNT VECTOR(tf-idf)\n",
      "NGRAM Key: 1 for UNIGRAMS, 2 for BIGRAMS, 3 for TRIGRAMS, (1,2) for (UNIGRAMS & BIGRAMS), (1,3) for (UNIGRAMS,BIGRAMS, & TRIGRAMS)\n",
      "\n",
      "Best Hyperparameters: VECTOR TYPE:  0 , MAX FEATURES:  900 , NGRAM:  (1, 2)\n",
      "Best One-Shot Accuracy:  66.53279785809906\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "vector_types = [0, 1, 2] # 0 for COUNT VECTOR, 1 for BINARY COUNT VECTOR, 2 for NORMALIZED COUNT VECTOR(tf-idf)\n",
    "max_features = [300, 500, 700, 800, 900]\n",
    "ngram_types = [(1,1), (2,2), (3,3), (1,2), (1,3)] # (1,1) for UNIGRAMS, (2,2) for BIGRAMS, (3,3) for TRIGRAMS, (1,2) for (UNIGRAMS,BIGRAMS), (1,3) for (UNIGRAMS,TRIGRAMS) \n",
    "\n",
    "# Optimal Result Initialization\n",
    "best_acc=0\n",
    "vector_type_best=0\n",
    "max_feature_best=0\n",
    "ngram_type_best=0\n",
    "best_assocMem=[]\n",
    "itemMem_best=[]\n",
    "Train_best=[]\n",
    "Test_best=[]\n",
    "\n",
    "# Generate Item Memories Once for Each Max Features Option\n",
    "ItemMem_300, ItemMem_500, ItemMem_700, ItemMem_800, ItemMem_900=[], [], [], [], []\n",
    "for feat_num in max_features:\n",
    "    if feat_num==300:\n",
    "        ItemMem_300 = itemMemGen(dim=HV_dim, num_char=feat_num)\n",
    "    elif feat_num==500:\n",
    "        ItemMem_500 = itemMemGen(dim=HV_dim, num_char=feat_num)\n",
    "    elif feat_num==700:\n",
    "        ItemMem_700 = itemMemGen(dim=HV_dim, num_char=feat_num)\n",
    "    elif feat_num==800:\n",
    "        ItemMem_800 = itemMemGen(dim=HV_dim, num_char=feat_num)\n",
    "    elif feat_num==900:\n",
    "        ItemMem_900 = itemMemGen(dim=HV_dim, num_char=feat_num)\n",
    "\n",
    "# Generate Table Initialization\n",
    "table_data=[]\n",
    "col_names = [\"VECTOR TYPE\", \"MAX FEATURES\", \"NGRAM\", \"ONE-SHOT ACCURACY\", \"TRAINING TIME (s)\", \"NUMBER OF TRAINING SAMPLES\", \"TESTING TIME (s)\", \"NUMBER OF TESTING SAMPLES\"]\n",
    "\n",
    "for vector_type in vector_types:\n",
    "    for max_feat in max_features:\n",
    "        \n",
    "        # Set ItemMem to Proper Generated ItemMeme\n",
    "        if max_feat==300:\n",
    "            itemMem = ItemMem_300\n",
    "        elif max_feat==500:\n",
    "            itemMem = ItemMem_500\n",
    "        elif max_feat==700:\n",
    "            itemMem = ItemMem_700\n",
    "        elif max_feat==800:\n",
    "            itemMem = ItemMem_800\n",
    "        elif max_feat==900:\n",
    "            itemMem = ItemMem_900\n",
    "\n",
    "        for ngram in ngram_types:\n",
    "            \n",
    "            # Build Vectorizer With Desired Hyperparameters\n",
    "            if (vector_type==0):\n",
    "                cnt = CountVectorizer(analyzer=\"word\", ngram_range=ngram, max_features=max_feat)\n",
    "            elif(vector_type==1):\n",
    "                cnt = CountVectorizer(analyzer=\"word\", ngram_range=ngram, max_features=max_feat, binary=True)\n",
    "            elif (vector_type==2):\n",
    "                cnt = TfidfVectorizer(analyzer=\"word\", ngram_range=ngram, max_features=max_feat) \n",
    "            \n",
    "            # Fit Vectorizer and Transform Training/Testing Sets\n",
    "            TrainXdf_temp = cnt.fit_transform(TrainXdf).toarray()\n",
    "            TestXdf_temp = cnt.transform(TestXdf).toarray()\n",
    "\n",
    "            # Train Model (i.e. Generate Model's Associative Memory)\n",
    "            t0=time.time()\n",
    "            assocMem = train(TrainXdf_temp, TrainYdf, itemMem, HV_dim, len(sent_count))\n",
    "            t1=time.time()\n",
    "            train_time = t1-t0\n",
    "\n",
    "            # One-Shot Training Results\n",
    "            t0=time.time()\n",
    "            one_shot_accuracy=test(itemMem, assocMem, TestXdf_temp, TestYdf)\n",
    "            t1=time.time()\n",
    "            test_time = t1-t0\n",
    "\n",
    "            # Add Data to Table\n",
    "            data = [vector_type, max_feat, ngram, one_shot_accuracy, train_time, len(TrainYdf), test_time, len(TestYdf)]\n",
    "            table_data.append(data)\n",
    "\n",
    "            if one_shot_accuracy>best_acc:\n",
    "                best_acc=one_shot_accuracy\n",
    "                best_assocMem=assocMem\n",
    "                itemMem_best=itemMem\n",
    "                Train_best=TrainXdf_temp\n",
    "                Test_best=TestXdf_temp\n",
    "                \n",
    "                cnt_best=cnt\n",
    "                vector_type_best=vector_type\n",
    "                max_feature_best=max_feat\n",
    "                ngram_type_best=ngram\n",
    "\n",
    "# Get Necessary Components for Best Model\n",
    "assocMem=best_assocMem\n",
    "itemMem=itemMem_best\n",
    "TrainXdf=Train_best\n",
    "TestXdf=Test_best\n",
    "\n",
    "# Save Results to File\n",
    "df=pd.DataFrame(table_data, columns=col_names)\n",
    "filepath=\"./Results/HyperparameterResults/VECTORIZED_\" + str(dataset) +\".csv\"\n",
    "df.to_csv(filepath)\n",
    "            \n",
    "print(\"Results of Various Hyperparameter Sets:\")\n",
    "print(tabulate(table_data, headers=col_names, tablefmt=\"simple\"))\n",
    "print(\"\\nVector Type Key: 0 for COUNT VECTOR, 1 for BINARY COUNT VECTOR, 2 for NORMALIZED COUNT VECTOR(tf-idf)\")\n",
    "print(\"NGRAM Key: 1 for UNIGRAMS, 2 for BIGRAMS, 3 for TRIGRAMS, (1,2) for (UNIGRAMS & BIGRAMS), (1,3) for (UNIGRAMS,BIGRAMS, & TRIGRAMS)\\n\")\n",
    "print(\"Best Hyperparameters: VECTOR TYPE: \", vector_type_best, \", MAX FEATURES: \", max_feature_best, \", NGRAM: \", ngram_type_best)\n",
    "print(\"Best One-Shot Accuracy: \", best_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retraining Optimal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Retraining Model:  20  Epochs --------\n",
      "Epoch  1 :  63.31994645247657\n",
      "Epoch  2 :  61.24497991967871\n",
      "Epoch  3 :  61.31191432396251\n",
      "Epoch  4 :  62.51673360107095\n",
      "Epoch  5 :  62.58366800535475\n",
      "Epoch  6 :  59.70548862115127\n",
      "Epoch  7 :  62.44979919678715\n",
      "Epoch  8 :  61.37884872824632\n",
      "Epoch  9 :  61.31191432396251\n",
      "Epoch  10 :  62.51673360107095\n",
      "Epoch  11 :  61.64658634538153\n",
      "Epoch  12 :  61.71352074966533\n",
      "Epoch  13 :  61.57965194109772\n",
      "Epoch  14 :  63.05220883534136\n",
      "Epoch  15 :  62.784471218206164\n",
      "Epoch  16 :  61.84738955823293\n",
      "Epoch  17 :  63.31994645247657\n",
      "Epoch  18 :  63.58768406961178\n",
      "Epoch  19 :  62.65060240963856\n",
      "Epoch  20 :  63.186077643908966\n"
     ]
    }
   ],
   "source": [
    "### Function: VECTORIZED HDC Re-Training Function that creates a New Associative Memory for the Model\n",
    "    ## Inputs: X:           Training Samples\n",
    "    ##         Y:           Outputs of Training Samples\n",
    "    ##         itemMem:     Generated Item Memory\n",
    "    ##         assocMem:    Associative Memory of Current Model\n",
    "    ##         alpha:       Learning Rate Parameter\n",
    "    ## Output: assocMem: New Associative Memory\n",
    "def retrain(X, Y, itemMem, assocMem, alpha):\n",
    "    sample_index = 0\n",
    "    for sample in X:\n",
    "        sample_HV = encode(sample, itemMem)\n",
    "        prediction = get_prediction(assocMem, sample_HV)\n",
    "        if prediction != Y[sample_index]:\n",
    "            assocMem[Y[sample_index]] = np.add(assocMem[Y[sample_index]], alpha * sample_HV)\n",
    "            assocMem[prediction] = np.subtract(assocMem[prediction], alpha * sample_HV)\n",
    "        sample_index += 1\n",
    "    return assocMem\n",
    "\n",
    "# Re-Train Optimal Model\n",
    "all_accuracies=[]\n",
    "num_epochs = 20\n",
    "print('-------- Retraining Model: ', num_epochs, ' Epochs --------')\n",
    "for epoch in range(num_epochs):\n",
    "    assocMem = retrain(TrainXdf, TrainYdf, itemMem, assocMem, alpha = num_epochs - epoch)\n",
    "    acc = test(itemMem, assocMem, TestXdf, TestYdf)\n",
    "    print('Epoch ', (epoch+1), ': ', acc)\n",
    "    all_accuracies.append(acc)\n",
    "\n",
    "# Save Results to File\n",
    "filepath=\"./Results/EpochResults/VECTORIZED_\" + str(dataset) + \".csv\"\n",
    "np.savetxt(filepath, np.array(all_accuracies), delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "071eaeccc96c6410cecdb330bf8e8ae0267d24b86e05481c728d399cbe7cbc33"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('aml': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
